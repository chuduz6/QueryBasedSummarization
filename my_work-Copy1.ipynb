{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import json\n",
    "import io\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# this loop is needed to reset the flags to that notebook won't throw duplicate flags error\n",
    "from absl import flags\n",
    "for name in list(flags.FLAGS):\n",
    "  delattr(flags.FLAGS, name)\n",
    "\n",
    "# Dictionary parameters\n",
    "tf.app.flags.DEFINE_string(\"doc_dict_path\", \"doc_dict.txt\", \"Document Dictionary output.\")\n",
    "tf.app.flags.DEFINE_string(\"sum_dict_path\", \"sum_dict.txt\", \"Summary Dictionary output.\")\n",
    "tf.app.flags.DEFINE_boolean(\"create_dict_flag\", False, \"Whether to create new dictionary or not \")\n",
    "tf.app.flags.DEFINE_integer(\"doc_vocab_size\", 30000, \"Document vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"sum_vocab_size\", 10000, \"Summary vocabulary size.\")\n",
    "tf.app.flags.DEFINE_float(\"train_test_split\", 0.02, \"Test Split ratio\")\n",
    "tf.app.flags.DEFINE_boolean(\"pretrained_embeddings\", True, \"Whether to look up pre-trained embedding for not \")\n",
    "tf.app.flags.DEFINE_string(\"embedding_path\", \"glove.twitter.27B.100d.txt\", \"Embedding path\")\n",
    "\n",
    "\n",
    "# needed to get rid of missing f parameter\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "# Optimization Parameters\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 400, \"Size of hidden layers.\")\n",
    "tf.app.flags.DEFINE_integer(\"embsize\", 100, \"Size of embedding.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 1, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient\", 1.0, \"Clip gradients l2 norm to this range.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 5, \"Batch size in training / beam size in testing.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train\", 0, \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"max_epochs\", 250, \"Maximum training iterations.\")\n",
    "\n",
    "# Data Directory Paramters\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"data1.json\", \"Data directory\")\n",
    "tf.app.flags.DEFINE_string(\"test_file\", \"data_sample_test.txt\", \"Test filename.\")\n",
    "\n",
    "# Output Data Directory Parameters\n",
    "tf.app.flags.DEFINE_string(\"test_output\", \"test_output.txt\", \"Test output.\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"model\", \"Training directory.\")\n",
    "tf.app.flags.DEFINE_string(\"tfboard\", \"tfboard\", \"Tensorboard log directory.\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_print\", 50, \"Training steps between printing.\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_validation\", 1000, \"Training steps between validations.\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 750, \"Training steps between checkpoints.\")\n",
    "tf.app.flags.DEFINE_boolean(\"load_checkpoint\", False, \"Flag to whether load the checkpoint or not\")\n",
    "\n",
    "\n",
    "# Progam Running Mode: Train or decode\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False, \"Set to True for testing.\")\n",
    "tf.app.flags.DEFINE_boolean(\"geneos\", True, \"Do not generate EOS. \")\n",
    "\n",
    "tf.app.flags.DEFINE_integer('seed',           3435, 'random number generator seed')\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO,format=\"%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\",datefmt='%b %d %H:%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARK_PAD = \"<PAD>\"\n",
    "MARK_UNK = \"<UNK>\"\n",
    "MARK_EOS = \"<EOS>\"\n",
    "MARK_GO = \"<GO>\"\n",
    "MARKS = [MARK_PAD, MARK_UNK, MARK_EOS, MARK_GO]\n",
    "ID_PAD = 0\n",
    "ID_UNK = 1\n",
    "ID_EOS = 2\n",
    "ID_GO = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path, max_vocab=None):\n",
    "    logging.info(\"Try load dict from {}.\".format(dict_path))\n",
    "    try:\n",
    "        dict_file = open(dict_path)\n",
    "        dict_data = dict_file.readlines()\n",
    "        dict_file.close()\n",
    "    except:\n",
    "        logging.info(\"Load dict {dict} failed, create later.\".format(dict=dict_path))\n",
    "        return None\n",
    "\n",
    "    dict_data = list(map(lambda x: x.split(), dict_data))\n",
    "    if max_vocab:\n",
    "        dict_data = list(filter(lambda x: int(x[0]) < max_vocab, dict_data))\n",
    "    tok2id = dict(map(lambda x: (x[1], int(x[0])), dict_data))\n",
    "    id2tok = dict(map(lambda x: (int(x[0]), x[1]), dict_data))\n",
    "    logging.info(\"Load dict {} with {} words.\".format(dict_path, len(tok2id)))\n",
    "    return (tok2id, id2tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(dict_path, corpus, max_vocab=None):\n",
    "    logging.info(\"Create dict {}.\".format(dict_path))\n",
    "    counter = {}\n",
    "    counter2 = 0\n",
    "    for line in corpus:\n",
    "        for word in line:\n",
    "            try:\n",
    "                counter[word] += 1\n",
    "            except:\n",
    "                counter[word] = 1\n",
    "\n",
    "    for mark_t in MARKS:\n",
    "        if mark_t in counter:\n",
    "            del counter[mark_t]\n",
    "            logging.warning(\"{} appears in corpus.\".format(mark_t))\n",
    "\n",
    "    counter = list(counter.items())\n",
    "    counter.sort(key=lambda x: -x[1])\n",
    "    words = list(map(lambda x: x[0], counter))\n",
    "    words = [MARK_PAD, MARK_UNK, MARK_EOS, MARK_GO] + words\n",
    "    if max_vocab:\n",
    "        words = words[:max_vocab]\n",
    "\n",
    "    tok2id = dict()\n",
    "    id2tok = dict()\n",
    "    with open(dict_path, 'w') as dict_file:\n",
    "        for idx, tok in enumerate(words):\n",
    "            print(idx, tok, file=dict_file)\n",
    "            tok2id[tok] = idx\n",
    "            id2tok[idx] = tok\n",
    "\n",
    "    logging.info(\"Create dict {} with {} words.\".format(dict_path, len(words)))\n",
    "    return (tok2id, id2tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_map2id(data, tok2id):\n",
    "    ret = []\n",
    "    unk = 0\n",
    "    tot = 0\n",
    "    for doc in data:\n",
    "        tmp = []\n",
    "        for word in doc:\n",
    "            tot += 1\n",
    "            try:\n",
    "                tmp.append(tok2id[word])\n",
    "            except:\n",
    "                tmp.append(ID_UNK)\n",
    "                unk +=1\n",
    "        ret.append(tmp)\n",
    "    print (\"TOTAL :\", tot, \" UNK :\", unk)\n",
    "    return ret, (tot - unk)/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_map2tok(sen, id2tok):\n",
    "\treturn list(map(lambda x: id2tok[x], sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:43 <ipython-input-9-c23af9b8db75>[line:2] INFO Load document from data1.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"Load document from {}.\".format(FLAGS.data_dir))\n",
    "with io.open(FLAGS.data_dir, 'r', encoding='ascii', errors='ignore') as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "    docs = []\n",
    "    summaries = []\n",
    "    queries = []\n",
    "    for i in range (len(data['passages'])):\n",
    "        docs.append(' '.join([data['passages'][str(i)][j]['passage_text'] for j in range (len(data['passages'][str(i)]))]))\n",
    "        summaries.append(''.join(data['answers'][str(i)]))\n",
    "        queries.append(data['query'][str(i)])\n",
    "\n",
    "assert \tlen(docs) == len(queries)\n",
    "print (len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY : . what is a corporation?\n",
      "ANSWER : A corporation is a company or group of people authorized to act as a single entity and recognized as such in law.\n"
     ]
    }
   ],
   "source": [
    "#print (\"DOCS :\", docs[0])\n",
    "print (\"QUERY :\", queries[0])\n",
    "print (\"ANSWER :\", summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting docs...\n",
      "TIME TAKEN TO SPLIT DOCS:  57.51096987724304\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "    print (\"Splitting docs...\")\n",
    "\n",
    "    #print (\"BEFORE SPLITTING: \", docs[0])\n",
    "    now = time.time()\n",
    "    docs_splitted = list(map(lambda x: word_tokenize(x), docs))\n",
    "    docs_splitted = [[word.lower()for word in doc] for doc in docs_splitted]\n",
    "    del docs\n",
    "    print (\"TIME TAKEN TO SPLIT DOCS: \", time.time()-now)\n",
    "    #print (\"AFTER SPLITTING: \", docs[0])\n",
    "    print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting queries...\n",
      "DONE\n",
      "15391\n"
     ]
    }
   ],
   "source": [
    "print (\"Splitting queries...\")\n",
    "queries_splitted = list(map(lambda x: [y.lower() for y in word_tokenize(x)], queries)) \n",
    "del queries\n",
    "print (\"DONE\")\n",
    "print (len(queries_splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting summaries...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print (\"Splitting summaries...\")\n",
    "summaries_splitted = list(map(lambda x: [y.lower() for y in word_tokenize(x)], summaries)) \n",
    "del summaries\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:44 <ipython-input-5-680d20c1ecef>[line:2] INFO Try load dict from doc_dict.txt.\n",
      "Sep 07 00:44 <ipython-input-5-680d20c1ecef>[line:16] INFO Load dict doc_dict.txt with 30000 words.\n",
      "Sep 07 00:44 <ipython-input-5-680d20c1ecef>[line:2] INFO Try load dict from sum_dict.txt.\n",
      "Sep 07 00:44 <ipython-input-5-680d20c1ecef>[line:16] INFO Load dict sum_dict.txt with 10000 words.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dictionary...\n",
      "DONE\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print (\"Working on dictionary...\")\n",
    "if(FLAGS.create_dict_flag):\n",
    "    doc_dict = create_dict(FLAGS.doc_dict_path, docs_splitted+queries_splitted, FLAGS.doc_vocab_size)\n",
    "    sum_dict = create_dict(FLAGS.sum_dict_path, summaries_splitted, FLAGS.sum_vocab_size)\n",
    "else:\n",
    "    doc_dict = load_dict(FLAGS.doc_dict_path, FLAGS.doc_vocab_size)\n",
    "    sum_dict = load_dict(FLAGS.sum_dict_path, FLAGS.sum_vocab_size)\n",
    "\n",
    "print (\"DONE\")\n",
    "print (len(doc_dict))\n",
    "print (len(sum_dict))\n",
    "#print (sum_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to ids...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:44 <ipython-input-15-6f91af374cc1>[line:3] INFO Doc dict covers 96.73% words.\n",
      "Sep 07 00:44 <ipython-input-15-6f91af374cc1>[line:6] INFO Sum dict covers 93.24% words.\n",
      "Sep 07 00:44 <ipython-input-15-6f91af374cc1>[line:9] INFO Query dict covers 98.35% words.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL : 9099177  UNK : 297354\n",
      "TOTAL : 486727  UNK : 32914\n",
      "TOTAL : 86919  UNK : 1431\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print (\"Converting to ids...\")\n",
    "docid, cover = corpus_map2id(docs_splitted, doc_dict[0])\n",
    "logging.info(\"Doc dict covers {:.2f}% words.\".format(cover * 100))\n",
    "\n",
    "sumid, cover = corpus_map2id(summaries_splitted, sum_dict[0])\n",
    "logging.info(\"Sum dict covers {:.2f}% words.\".format(cover * 100))\n",
    "\n",
    "queryid, cover = corpus_map2id(queries_splitted, doc_dict[0])\n",
    "logging.info(\"Query dict covers {:.2f}% words.\".format(cover * 100))\n",
    "\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[5, 37, 11, 8, 1557, 63]\n",
      ". what is a corporation ?\n"
     ]
    }
   ],
   "source": [
    "print (len(queryid[0]))\n",
    "print (queryid[0])\n",
    "print (\" \".join(sen_map2tok(queryid[0], doc_dict[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chudu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Sep 07 00:44 textcleaner.py[line:37] INFO 'pattern' package not found; tag filters are not available for English\n",
      "Sep 07 00:44 dictionary.py[line:195] INFO adding document #0 to Dictionary(0 unique tokens: [])\n",
      "Sep 07 00:44 dictionary.py[line:202] INFO built Dictionary(12 unique tokens: ['response', 'trees', 'time', 'user', 'human']...) from 9 documents (total 29 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:51 glove2word2vec.py[line:105] INFO converting 1193514 vectors from glove.twitter.27B.100d.txt to C:\\Users\\chudu\\AppData\\Local\\Temp\\word2vec_format.vec\n",
      "Sep 07 00:51 utils_any2vec.py[line:170] INFO loading projection weights from C:\\Users\\chudu\\AppData\\Local\\Temp\\word2vec_format.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:52 utils_any2vec.py[line:232] INFO loaded (1193514, 100) matrix from C:\\Users\\chudu\\AppData\\Local\\Temp\\word2vec_format.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def get_word_vectors():\n",
    "    glove_file = FLAGS.embedding_path\n",
    "    word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
    "    glove2word2vec(glove_file, word2vec_file)\n",
    "    print(\"Loading Glove vectors...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "    return word_vectors\n",
    "word_vectors = get_word_vectors()\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def get_init_embedding(word_vectors, word_vocab, vocab_size):\n",
    "    word_vec_list = list()\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    for word, _ in word_vocab[0].items():\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "            success_count += 1\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([FLAGS.embsize], dtype=np.float32)\n",
    "            failure_count += 1\n",
    "        word_vec_list.append(word_vec)\n",
    "\n",
    "    word_vec_list[2] = np.random.normal(0, 1, FLAGS.embsize)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, FLAGS.embsize)\n",
    "    print (\"SUCCES COUNT: \", success_count, \" FAILURE COUNT: \", failure_count)\n",
    "    return np.array(word_vec_list)\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCES COUNT:  25251  FAILURE COUNT:  4749\n",
      "SUCCES COUNT:  9505  FAILURE COUNT:  495\n",
      "LOADED GLOVE VECTORS IN TIME:  0.23414158821105957\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "if FLAGS.pretrained_embeddings:\n",
    "    init_embeddings_doc = tf.constant(get_init_embedding(word_vectors, doc_dict, FLAGS.doc_vocab_size), dtype=tf.float32)\n",
    "    init_embeddings_sum = tf.constant(get_init_embedding(word_vectors, sum_dict, FLAGS.sum_vocab_size), dtype=tf.float32)\n",
    "print (\"LOADED GLOVE VECTORS IN TIME: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer = tf.contrib.layers.fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "class BiGRUModel(object):\n",
    "\n",
    "    def __init__(self,doc_dict, sum_dict, source_vocab_size,target_vocab_size, buckets, state_size, num_layers, embedding_size, max_gradient, batch_size, learning_rate, forward_only=False, dtype=tf.float32):\n",
    "\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.buckets = buckets\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        self.state_size = state_size\n",
    "        self.sum_dict = sum_dict\n",
    "        self.doc_dict = doc_dict\n",
    "\n",
    "        self.encoder_query_inputs = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n",
    "        self.encoder_doc_inputs = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n",
    "        self.decoder_inputs = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n",
    "        self.decoder_targets = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n",
    "        self.encoder_query_len = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.encoder_doc_len = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.beam_tok = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.prev_att = tf.placeholder(tf.float32, shape=[self.batch_size, state_size * 2])\n",
    "\n",
    "        encoder_fw_cell_doc = tf.contrib.rnn.GRUCell(state_size)\n",
    "        encoder_bw_cell_doc = tf.contrib.rnn.GRUCell(state_size)\n",
    "        encoder_fw_cell_query = tf.contrib.rnn.GRUCell(state_size)\n",
    "        encoder_bw_cell_query = tf.contrib.rnn.GRUCell(state_size)\n",
    "        decoder_cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "\n",
    "        if not forward_only:\n",
    "            encoder_fw_cell_doc = tf.contrib.rnn.DropoutWrapper(encoder_fw_cell_doc, output_keep_prob=0.50)\n",
    "            encoder_bw_cell_doc = tf.contrib.rnn.DropoutWrapper(encoder_bw_cell_doc, output_keep_prob=0.50)\n",
    "            encoder_fw_cell_query = tf.contrib.rnn.DropoutWrapper(encoder_fw_cell_query, output_keep_prob=0.50)\n",
    "            encoder_bw_cell_query = tf.contrib.rnn.DropoutWrapper(encoder_bw_cell_query, output_keep_prob=0.50)\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, output_keep_prob=0.50)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"seq2seq\", dtype=dtype):\n",
    "            with tf.name_scope(\"embedding_initializer\"):\n",
    "                if not forward_only and FLAGS.pretrained_embeddings is not None:\n",
    "                    init_embeddings_doc = tf.constant(get_init_embedding(word_vectors, doc_dict, FLAGS.doc_vocab_size), dtype=tf.float32)\n",
    "                    init_embeddings_sum = tf.constant(get_init_embedding(word_vectors, sum_dict, FLAGS.sum_vocab_size), dtype=tf.float32)\n",
    "                else:\n",
    "                    init_embeddings_doc = tf.random_uniform([self.source_vocab_size, self.embedding_size], -1.0, 1.0)\n",
    "                    init_embeddings_sum = tf.random_uniform([self.target_vocab_size, self.embedding_size], -1.0, 1.0)\n",
    "                    print (\"FORWARD OR NO EMBEDDING PATH\")\n",
    "                \n",
    "            with tf.variable_scope(\"encoder_query\"):\n",
    "                encoder_query_emb = tf.get_variable(\"embedding_query\", initializer=init_embeddings_doc)\n",
    "                encoder_query_inputs_emb = tf.nn.embedding_lookup(encoder_query_emb, self.encoder_query_inputs)\n",
    "                encoder_query_outputs, encoder_query_states = tf.nn.bidirectional_dynamic_rnn(encoder_fw_cell_query, encoder_bw_cell_query, encoder_query_inputs_emb, sequence_length=self.encoder_query_len, dtype=dtype)\n",
    "                    \n",
    "            with tf.variable_scope(\"encoder_doc\"):\n",
    "                encoder_doc_emb = tf.get_variable(\"embedding_doc\", initializer=init_embeddings_doc)\n",
    "                encoder_doc_inputs_emb = tf.nn.embedding_lookup(encoder_doc_emb, self.encoder_doc_inputs)\n",
    "                encoder_doc_outputs, encoder_doc_states = tf.nn.bidirectional_dynamic_rnn(encoder_fw_cell_doc, encoder_bw_cell_doc, encoder_doc_inputs_emb, sequence_length=self.encoder_doc_len, dtype=dtype)\n",
    "                    \n",
    "            with tf.variable_scope(\"init_state_query\"):\n",
    "                init_state_query = fc_layer(tf.concat(encoder_query_states, 1), state_size)\n",
    "                self.init_state_query = init_state_query\n",
    "                self.init_state_query.set_shape([self.batch_size, state_size])\n",
    "                self.att_states_query = tf.concat(encoder_query_outputs, 2)\n",
    "                self.att_states_query.set_shape([self.batch_size, None, state_size*2])\n",
    "                \n",
    "            with tf.variable_scope(\"init_state_doc\"):\n",
    "                init_state_doc = fc_layer( tf.concat(encoder_doc_states, 1), state_size)\n",
    "                self.init_state_doc = init_state_doc\n",
    "                self.init_state_doc.set_shape([self.batch_size, state_size])\n",
    "                self.att_states_doc = tf.concat(encoder_doc_outputs, 2)\n",
    "                self.att_states_doc.set_shape([self.batch_size, None, state_size*2])\n",
    "                \n",
    "            with tf.variable_scope(\"attention_query\"):\n",
    "                attention_mechanism_query = tf.contrib.seq2seq.BahdanauAttention(state_size, self.att_states_query, self.encoder_query_len)\n",
    "                attention_state_query = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism_query, state_size * 2)              \n",
    "                \n",
    "            with tf.variable_scope(\"attention_doc\"):\n",
    "                attention_mechanism_doc = tf.contrib.seq2seq.BahdanauAttention(state_size, self.att_states_doc, self.encoder_doc_len)                \n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(attention_state_query, attention_mechanism_doc, state_size * 2)               \n",
    "\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "\n",
    "                decoder_emb = tf.get_variable(\"embedding\", initializer= init_embeddings_sum)\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, target_vocab_size)\n",
    "                \n",
    "                if not forward_only:\n",
    "                    decoder_inputs_emb = tf.nn.embedding_lookup(decoder_emb, self.decoder_inputs)\n",
    "                    helper = tf.contrib.seq2seq.TrainingHelper(decoder_inputs_emb, self.decoder_len)\n",
    "                    decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size))\n",
    "                    outputs = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "\n",
    "                    outputs_logits = outputs[0].rnn_output\n",
    "                    self.outputs = outputs_logits\n",
    "                    #print (\"SHAPE OF OUTPUTS: \", tf.shape(outputs_logits, out_type=tf.int32 ))\n",
    "                    #print (\"SHAPE OF TARGETS: \", tf.shape(self.decoder_targets, out_type=tf.int32))\n",
    "                    weights = tf.sequence_mask(self.decoder_len, dtype=tf.float32)\n",
    "                    loss_t = tf.contrib.seq2seq.sequence_loss(outputs_logits, self.decoder_targets, weights, average_across_timesteps=False, average_across_batch=False)\n",
    "                    self.loss = tf.reduce_sum(loss_t)/FLAGS.batch_size                    \n",
    "                    predictions = tf.cast(tf.argmax(outputs_logits, axis=2), tf.int32) \n",
    "                    self.accuracy = tf.contrib.metrics.accuracy(predictions, self.decoder_targets)\n",
    "\n",
    "                    params = tf.trainable_variables()\n",
    "                    opt = tf.train.AdadeltaOptimizer(self.learning_rate, epsilon=1e-4)\n",
    "                    gradients = tf.gradients(self.loss, params)\n",
    "                    clipped_gradients, norm = \\\n",
    "                        tf.clip_by_global_norm(gradients, max_gradient)\n",
    "                    self.updates = opt.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "                    tf.summary.scalar('loss', self.loss)\n",
    "                    #tf.summary.scalar('accuracy', self.accuracy)\n",
    "                else:\n",
    "                    self.loss = tf.constant(0)\n",
    "                    self.accuracy = tf.constant(0)\n",
    "                    with tf.variable_scope(\"proj\") as scope:\n",
    "                        output_fn = lambda x: fc_layer(x, target_vocab_size, scope=scope)\n",
    "\n",
    "                    st_toks = tf.convert_to_tensor([ID_GO]*batch_size, dtype=tf.int32)\n",
    "\n",
    "                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_emb, st_toks, ID_EOS)\n",
    "\n",
    "                    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, wrapper_state)\n",
    "\n",
    "                    outputs = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "\n",
    "                    self.outputs = outputs[0].rnn_output\n",
    "\n",
    "                    # single step decode for beam search\n",
    "                    with tf.variable_scope(\"decoder_beam\"):\n",
    "                        beam_emb = tf.nn.embedding_lookup(decoder_emb, self.beam_tok)\n",
    "                        self.beam_outputs, self.beam_nxt_state, _, _ = decoder.step(0, beam_emb, wrapper_state)\n",
    "                        self.beam_logsoftmax = tf.nn.log_softmax(self.beam_outputs[0])\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=20)\n",
    "        self.summary_merge = tf.summary.merge_all()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self,session,encoder_doc_inputs,encoder_query_inputs,decoder_inputs,encoder_doc_len,encoder_query_len,decoder_len,forward_only,summary_writer=None):\n",
    "\n",
    "        # dim fit is important for sequence_mask\n",
    "        # TODO better way to use sequence_mask\n",
    "        if encoder_query_inputs.shape[1] != max(encoder_query_len):\n",
    "            raise ValueError(\"encoder_query_inputs and encoder_query_len does not fit\")\n",
    "        if encoder_doc_inputs.shape[1] != max(encoder_doc_len):\n",
    "            raise ValueError(\"encoder_doc_inputs and encoder_doc_len does not fit\")\n",
    "        if not forward_only and \\\n",
    "            decoder_inputs.shape[1] != max(decoder_len) + 1:\n",
    "            raise ValueError(\"decoder_inputs and decoder_len does not fit\")\n",
    "        input_feed = {}\n",
    "        input_feed[self.encoder_query_inputs] = encoder_query_inputs\n",
    "        input_feed[self.encoder_doc_inputs] = encoder_doc_inputs\n",
    "        input_feed[self.decoder_inputs] = decoder_inputs[:, :-1]\n",
    "        input_feed[self.decoder_targets] = decoder_inputs[:, 1:]\n",
    "        input_feed[self.encoder_query_len] = encoder_query_len\n",
    "        input_feed[self.encoder_doc_len] = encoder_doc_len\n",
    "        input_feed[self.decoder_len] = decoder_len\n",
    "        input_feed[self.prev_att] = np.zeros([self.batch_size, 2 * self.state_size])\n",
    "        #print (\"FINE TILL HERE\")\n",
    "\n",
    "        if forward_only:\n",
    "            output_feed = [self.loss, self.accuracy, self.outputs]\n",
    "        else:\n",
    "            output_feed = [self.loss, self.accuracy, self.updates]\n",
    "\n",
    "        if summary_writer:\n",
    "            output_feed += [self.summary_merge, self.global_step]\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "\n",
    "        if summary_writer:\n",
    "            summary_writer.add_summary(outputs[3], outputs[4])\n",
    "        return outputs[:3]\n",
    "\n",
    "    def step_beam(self,\n",
    "                  session,\n",
    "                  encoder_doc_inputs, encoder_query_inputs,\n",
    "                  encoder_doc_len, encoder_query_len,\n",
    "                  max_len=12,\n",
    "                  geneos=True):\n",
    "        print (\"CAME HERE ERROR\")\n",
    "        ret = []\n",
    "        return ret\n",
    "\n",
    "\n",
    "\n",
    "    def add_pad(self, data, fixlen):\n",
    "        data = map(lambda x: x + [ID_PAD] * (fixlen - len(x)), data)\n",
    "        data = list(data)\n",
    "        return np.asarray(data)\n",
    "    \n",
    "    def batchify (self, data_set, _buckets):\n",
    "        batched_data_set = []\n",
    "        encoder_query_inputs, encoder_doc_inputs, decoder_inputs = [], [], []\n",
    "        encoder_query_len, encoder_doc_len, decoder_len = [], [], []\n",
    "        num_data = 0\n",
    "        counter = 0\n",
    "        for bucket_id in range (len(_buckets)):\n",
    "            if(len(data_set[bucket_id])==0):\n",
    "                continue\n",
    "            for j in range(len(data_set[bucket_id])):\n",
    "                counter += 1\n",
    "                encoder_doc_input, encoder_query_input, decoder_input = data_set[bucket_id][j]\n",
    "                encoder_doc_inputs.append(encoder_doc_input)\n",
    "                encoder_doc_len.append(len(encoder_doc_input))            \n",
    "                encoder_query_inputs.append(encoder_query_input)\n",
    "                encoder_query_len.append(len(encoder_query_input))\n",
    "                decoder_inputs.append(decoder_input)\n",
    "                decoder_len.append(len(decoder_input))\n",
    "                num_data += 1\n",
    "                \n",
    "                if(num_data == FLAGS.batch_size):\n",
    "                    num_data = 0\n",
    "                    batch_enc_doc_len = max(encoder_doc_len)\n",
    "                    batch_enc_query_len = max(encoder_query_len)\n",
    "                    batch_dec_len = max(decoder_len)\n",
    "                    encoder_doc_inputs = self.add_pad(encoder_doc_inputs, batch_enc_doc_len)\n",
    "                    encoder_query_inputs = self.add_pad(encoder_query_inputs, batch_enc_query_len)\n",
    "                    decoder_inputs = self.add_pad(decoder_inputs, batch_dec_len)\n",
    "                    encoder_doc_len = np.asarray(encoder_doc_len)\n",
    "                    encoder_query_len = np.asarray(encoder_query_len)\n",
    "                    decoder_len = np.asarray(decoder_len) - 1\n",
    "                    \n",
    "                    batched_data_set.append([encoder_doc_inputs, encoder_query_inputs, decoder_inputs, encoder_doc_len, encoder_query_len, decoder_len])\n",
    "                    \n",
    "                    encoder_query_inputs, encoder_doc_inputs, decoder_inputs = [], [], []\n",
    "                    encoder_query_len, encoder_doc_len, decoder_len = [], [], []\n",
    "        print (\"BATCHED COUNTER: \", counter)\n",
    "        print (\"BATCHED LENGTH: \", len(batched_data_set))\n",
    "        return batched_data_set\n",
    "\n",
    "print (\"DONE\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# We use a number of buckets for sampling\n",
    "_buckets = [(300,5,15), (300,10,20), (300,15,25), (300,20,40), (300,40,50), (350,5,15), (350,10,20), (350,15,25), (350,20,40), (350,40,50), (450,5,15), (450,10,30), (450,15,50), (450,20,100), (450,40,150), (550,5,15), (550,10,30), (550,15,60), (550,20,100), (550,40,150), (650,5,15), (650,10,30), (650,15,60), (650,20,100), (650,40,150), (750,5,15), (750,10,30), (750,15,60), (750,20,100), (750,40,240), (850,5,15), (850,10,30), (850,15,60), (850,20,100), (850,40,240), (1050,5,15), (1050,10,30), (1050,15,60), (1050,20,100), (1050,40,240), (1500,5,15), (1500,10,30), (1500,15,60), (1500,20,100), (1500,40,300), ]\n",
    "\n",
    "\n",
    "print (\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def create_bucket(source, query, target):\n",
    "    totalDocs = len(source)\n",
    "    print (\"TOTAL DOCS BEFORE BUCKETS: \", totalDocs)\n",
    "    data_set = [[] for _ in _buckets]\n",
    "    for s, q, t in zip(source, query, target):\n",
    "        t = [ID_GO] + t + [ID_EOS]\n",
    "        found = False\n",
    "        for bucket_id, (s_size, q_size, t_size) in enumerate(_buckets):\n",
    "            if len(s) <= s_size and len(q) <= q_size and len(t) <= t_size:\n",
    "                data_set[bucket_id].append([s, q, t])\n",
    "                found = True\n",
    "                break\n",
    "        if(found != True):\n",
    "            print (\"Didn't find bucket for {}, {}, {}\".format(len(s), len(q), len(t)))\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def create_model(session, doc_dict, sum_dict, forward_only):\n",
    "    \"\"\"Create model and initialize or load parameters in session.\"\"\"\n",
    "    dtype = tf.float32\n",
    "    model = BiGRUModel(doc_dict, sum_dict, FLAGS.doc_vocab_size, FLAGS.sum_vocab_size, _buckets, FLAGS.size,  FLAGS.num_layers, FLAGS.embsize, FLAGS.max_gradient,\n",
    "        FLAGS.batch_size,     FLAGS.learning_rate,       forward_only=forward_only,        dtype=dtype)\n",
    "    print (\"Loading Checkpoint: \", FLAGS.load_checkpoint)\n",
    "    if (FLAGS.load_checkpoint):        \n",
    "        ckpt = tf.train.latest_checkpoint(FLAGS.train_dir)\n",
    "        if ckpt:\n",
    "            #ckpt = ckpt.model_checkpoint_path\n",
    "            if ckpt and tf.train.checkpoint_exists(ckpt):\n",
    "                logging.info(\"Reading model parameters from %s\" % ckpt)\n",
    "                model.saver.restore(session, ckpt)\n",
    "            else:\n",
    "                logging.error(\"Don't have any checkpoints to load: %s\" % ckpt)\n",
    "    else:\n",
    "        logging.info(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model\n",
    "\n",
    "\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:7] INFO Preparing summarization data.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:25] INFO Creating 1 layers of 400 units.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Train\n",
      "SUCCES COUNT:  25251  FAILURE COUNT:  4749\n",
      "SUCCES COUNT:  9505  FAILURE COUNT:  495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:58 <ipython-input-29-78f466946508>[line:34] INFO Created model with fresh parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:30] INFO Create buckets.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (300, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (300, 10, 20) has 1 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (300, 15, 25) has 3 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (300, 20, 40) has 2 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (300, 40, 50) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (350, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (350, 10, 20) has 1 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (350, 15, 25) has 13 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (350, 20, 40) has 10 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (350, 40, 50) has 1 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (450, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (450, 10, 30) has 484 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (450, 15, 50) has 214 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (450, 20, 100) has 48 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (450, 40, 150) has 8 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (550, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (550, 10, 30) has 3146 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (550, 15, 60) has 2091 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (550, 20, 100) has 160 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (550, 40, 150) has 19 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (650, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (650, 10, 30) has 3286 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (650, 15, 60) has 2571 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (650, 20, 100) has 345 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (650, 40, 150) has 44 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (750, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (750, 10, 30) has 753 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (750, 15, 60) has 671 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (750, 20, 100) has 135 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (750, 40, 240) has 35 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (850, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (850, 10, 30) has 101 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (850, 15, 60) has 110 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (850, 20, 100) has 25 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (850, 40, 240) has 8 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1050, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1050, 10, 30) has 149 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1050, 15, 60) has 283 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1050, 20, 100) has 42 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1050, 40, 240) has 2 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1500, 5, 15) has 0 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1500, 10, 30) has 64 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1500, 15, 60) has 177 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1500, 20, 100) has 71 samples.\n",
      "Sep 07 00:58 <ipython-input-30-4c64aabca3a0>[line:40] INFO Train set bucket (1500, 40, 300) has 10 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL DOCS BEFORE BUCKETS:  308\n",
      "TOTAL DOCS BEFORE BUCKETS:  15083\n",
      "BATCHED COUNTER:  15083\n",
      "BATCHED LENGTH:  3016\n",
      "BATCHED COUNTER:  308\n",
      "BATCHED LENGTH:  61\n",
      "Time taken to save checkpoint:  3.6710524559020996\n",
      "Epoch: 0, GlobalStep: 1, step-time 3.00, Acc: 0.0000, Loss: 9.2126, Perpxty: 10023.00\n",
      "Epoch: 0, GlobalStep: 51, step-time 0.73, Acc: 0.0500, Loss: 6.4540, Perpxty: 635.22\n",
      "Epoch: 0, GlobalStep: 101, step-time 0.78, Acc: 0.0593, Loss: 6.3018, Perpxty: 545.55\n",
      "Epoch: 0, GlobalStep: 151, step-time 0.97, Acc: 0.0407, Loss: 6.9551, Perpxty: 1048.44\n",
      "Epoch: 0, GlobalStep: 201, step-time 0.94, Acc: 0.0966, Loss: 6.3832, Perpxty: 591.85\n",
      "Epoch: 0, GlobalStep: 251, step-time 0.95, Acc: 0.0897, Loss: 6.4147, Perpxty: 610.76\n",
      "Epoch: 0, GlobalStep: 301, step-time 0.92, Acc: 0.1478, Loss: 6.0470, Perpxty: 422.83\n",
      "Epoch: 0, GlobalStep: 351, step-time 0.92, Acc: 0.1200, Loss: 6.3606, Perpxty: 578.61\n",
      "Epoch: 0, GlobalStep: 401, step-time 0.95, Acc: 0.1231, Loss: 6.3291, Perpxty: 560.64\n",
      "Epoch: 0, GlobalStep: 451, step-time 0.94, Acc: 0.1417, Loss: 6.3867, Perpxty: 593.91\n",
      "Epoch: 0, GlobalStep: 501, step-time 0.97, Acc: 0.1462, Loss: 5.8260, Perpxty: 338.98\n",
      "Epoch: 0, GlobalStep: 551, step-time 1.00, Acc: 0.1600, Loss: 5.9459, Perpxty: 382.17\n",
      "Epoch: 0, GlobalStep: 601, step-time 0.94, Acc: 0.1636, Loss: 6.0184, Perpxty: 410.94\n",
      "Epoch: 0, GlobalStep: 651, step-time 0.95, Acc: 0.1172, Loss: 5.8719, Perpxty: 354.94\n",
      "Epoch: 0, GlobalStep: 701, step-time 0.98, Acc: 0.1586, Loss: 5.6506, Perpxty: 284.46\n",
      "Time taken to save checkpoint:  3.5616509914398193\n",
      "Epoch: 0, GlobalStep: 751, step-time 1.06, Acc: 0.1172, Loss: 6.0329, Perpxty: 416.91\n",
      "Epoch: 0, GlobalStep: 801, step-time 1.06, Acc: 0.1378, Loss: 6.1265, Perpxty: 457.82\n",
      "Epoch: 0, GlobalStep: 851, step-time 1.08, Acc: 0.0654, Loss: 6.5656, Perpxty: 710.24\n",
      "Epoch: 0, GlobalStep: 901, step-time 1.05, Acc: 0.0978, Loss: 6.1487, Perpxty: 468.10\n",
      "Epoch: 0, GlobalStep: 951, step-time 1.11, Acc: 0.1022, Loss: 6.2113, Perpxty: 498.36\n",
      "Epoch: 0, GlobalStep: 1001, step-time 1.16, Acc: 0.0778, Loss: 6.2765, Perpxty: 531.91\n",
      "Epoch: 0, GlobalStep: 1051, step-time 1.06, Acc: 0.1106, Loss: 5.8906, Perpxty: 361.61\n",
      "Epoch: 0, GlobalStep: 1101, step-time 1.05, Acc: 0.1067, Loss: 6.5542, Perpxty: 702.16\n",
      "Epoch: 0, GlobalStep: 1151, step-time 1.06, Acc: 0.1619, Loss: 5.7579, Perpxty: 316.68\n",
      "Epoch: 0, GlobalStep: 1201, step-time 1.22, Acc: 0.0964, Loss: 6.1320, Perpxty: 460.34\n",
      "Epoch: 0, GlobalStep: 1251, step-time 1.03, Acc: 0.1692, Loss: 5.7116, Perpxty: 302.36\n",
      "Epoch: 0, GlobalStep: 1301, step-time 1.17, Acc: 0.1655, Loss: 5.6573, Perpxty: 286.38\n",
      "Epoch: 0, GlobalStep: 1351, step-time 1.06, Acc: 0.1360, Loss: 6.0020, Perpxty: 404.22\n",
      "Epoch: 0, GlobalStep: 1401, step-time 1.16, Acc: 0.1259, Loss: 5.7337, Perpxty: 309.12\n",
      "Epoch: 0, GlobalStep: 1451, step-time 1.08, Acc: 0.1714, Loss: 5.7255, Perpxty: 306.59\n",
      "Time taken to save checkpoint:  3.280487537384033\n",
      "Epoch: 0, GlobalStep: 1501, step-time 1.11, Acc: 0.1407, Loss: 5.6696, Perpxty: 289.93\n",
      "Epoch: 0, GlobalStep: 1551, step-time 1.00, Acc: 0.1280, Loss: 6.1922, Perpxty: 488.94\n",
      "Epoch: 0, GlobalStep: 1601, step-time 1.16, Acc: 0.1857, Loss: 5.4437, Perpxty: 231.30\n",
      "Epoch: 0, GlobalStep: 1651, step-time 1.17, Acc: 0.1786, Loss: 5.8818, Perpxty: 358.47\n",
      "Epoch: 0, GlobalStep: 1701, step-time 1.14, Acc: 0.2083, Loss: 4.6740, Perpxty: 107.13\n",
      "Epoch: 0, GlobalStep: 1751, step-time 1.17, Acc: 0.1357, Loss: 5.8205, Perpxty: 337.13\n",
      "Epoch: 0, GlobalStep: 1801, step-time 1.14, Acc: 0.1310, Loss: 5.4568, Perpxty: 234.34\n",
      "Epoch: 0, GlobalStep: 1851, step-time 1.09, Acc: 0.1440, Loss: 5.7644, Perpxty: 318.73\n",
      "Epoch: 0, GlobalStep: 1901, step-time 1.17, Acc: 0.1120, Loss: 5.9461, Perpxty: 382.28\n",
      "Epoch: 0, GlobalStep: 1951, step-time 1.30, Acc: 0.1774, Loss: 5.9025, Perpxty: 365.95\n",
      "Epoch: 0, GlobalStep: 2001, step-time 1.23, Acc: 0.1702, Loss: 5.7198, Perpxty: 304.84\n",
      "Epoch: 0, GlobalStep: 2051, step-time 1.23, Acc: 0.1393, Loss: 5.9266, Perpxty: 374.87\n",
      "Epoch: 0, GlobalStep: 2101, step-time 1.27, Acc: 0.0906, Loss: 6.1857, Perpxty: 485.76\n",
      "Epoch: 0, GlobalStep: 2151, step-time 1.16, Acc: 0.1282, Loss: 5.3753, Perpxty: 215.99\n",
      "Epoch: 0, GlobalStep: 2201, step-time 1.27, Acc: 0.1511, Loss: 5.5311, Perpxty: 252.43\n",
      "Time taken to save checkpoint:  3.37424373626709\n",
      "Epoch: 0, GlobalStep: 2251, step-time 1.25, Acc: 0.1333, Loss: 5.9667, Perpxty: 390.21\n",
      "Epoch: 0, GlobalStep: 2301, step-time 1.28, Acc: 0.1000, Loss: 5.7098, Perpxty: 301.83\n",
      "Epoch: 0, GlobalStep: 2351, step-time 1.16, Acc: 0.1476, Loss: 5.5999, Perpxty: 270.39\n",
      "Epoch: 0, GlobalStep: 2401, step-time 1.12, Acc: 0.1600, Loss: 5.6266, Perpxty: 277.72\n",
      "Epoch: 0, GlobalStep: 2451, step-time 1.44, Acc: 0.0883, Loss: 6.1865, Perpxty: 486.12\n",
      "Epoch: 0, GlobalStep: 2501, step-time 1.23, Acc: 0.1143, Loss: 5.4489, Perpxty: 232.51\n",
      "Epoch: 0, GlobalStep: 2551, step-time 1.28, Acc: 0.1857, Loss: 5.4815, Perpxty: 240.22\n",
      "Epoch: 0, GlobalStep: 2601, step-time 1.25, Acc: 0.1929, Loss: 5.2564, Perpxty: 191.79\n",
      "Epoch: 0, GlobalStep: 2651, step-time 1.42, Acc: 0.1059, Loss: 6.1224, Perpxty: 455.96\n",
      "Epoch: 0, GlobalStep: 2701, step-time 1.37, Acc: 0.1617, Loss: 5.5827, Perpxty: 265.80\n",
      "Epoch: 0, GlobalStep: 2751, step-time 1.41, Acc: 0.1964, Loss: 5.3135, Perpxty: 203.06\n",
      "Epoch: 0, GlobalStep: 2801, step-time 1.66, Acc: 0.1239, Loss: 5.9646, Perpxty: 389.41\n",
      "Epoch: 0, GlobalStep: 2851, step-time 1.80, Acc: 0.0882, Loss: 5.9419, Perpxty: 380.67\n",
      "Epoch: 0, GlobalStep: 2901, step-time 1.89, Acc: 0.1867, Loss: 5.3472, Perpxty: 210.01\n",
      "Epoch: 0, GlobalStep: 2951, step-time 2.20, Acc: 0.0936, Loss: 6.0492, Perpxty: 423.79\n",
      "Time taken to save checkpoint:  3.514801502227783\n",
      "Epoch: 0, GlobalStep: 3001, step-time 2.56, Acc: 0.1082, Loss: 5.5556, Perpxty: 258.67\n",
      "at the end of epoch: 0\n",
      "Average train loss = 5.90492710, Average perplexity = 366.84048055\n",
      "Average train acc = 0.13073591\n",
      "validation loss = 5.85288653, perplexity = 348.23813146\n",
      "Average Validation acc = 0.11641576\n",
      "Time taken to save checkpoint:  3.8203299045562744\n",
      "Time taken to save checkpoint:  3.5696167945861816\n",
      "Epoch: 1, GlobalStep: 3017, step-time 0.59, Acc: 0.1154, Loss: 5.6209, Perpxty: 276.14\n",
      "Epoch: 1, GlobalStep: 3067, step-time 0.84, Acc: 0.2143, Loss: 5.4156, Perpxty: 224.89\n",
      "Epoch: 1, GlobalStep: 3117, step-time 0.80, Acc: 0.2000, Loss: 5.4578, Perpxty: 234.58\n",
      "Epoch: 1, GlobalStep: 3167, step-time 1.08, Acc: 0.1254, Loss: 6.1376, Perpxty: 462.96\n",
      "Epoch: 1, GlobalStep: 3217, step-time 0.98, Acc: 0.1310, Loss: 5.4190, Perpxty: 225.66\n",
      "Epoch: 1, GlobalStep: 3267, step-time 0.97, Acc: 0.2000, Loss: 5.7984, Perpxty: 329.76\n",
      "Epoch: 1, GlobalStep: 3317, step-time 1.03, Acc: 0.2000, Loss: 5.2137, Perpxty: 183.77\n",
      "Epoch: 1, GlobalStep: 3367, step-time 0.92, Acc: 0.1280, Loss: 5.6242, Perpxty: 277.04\n",
      "Epoch: 1, GlobalStep: 3417, step-time 1.02, Acc: 0.1846, Loss: 5.7833, Perpxty: 324.84\n",
      "Epoch: 1, GlobalStep: 3467, step-time 0.97, Acc: 0.1583, Loss: 5.7261, Perpxty: 306.77\n",
      "Epoch: 1, GlobalStep: 3517, step-time 0.95, Acc: 0.2308, Loss: 5.1868, Perpxty: 178.90\n",
      "Epoch: 1, GlobalStep: 3567, step-time 1.02, Acc: 0.1760, Loss: 5.3534, Perpxty: 211.33\n",
      "Epoch: 1, GlobalStep: 3617, step-time 0.94, Acc: 0.1909, Loss: 5.4792, Perpxty: 239.65\n",
      "Epoch: 1, GlobalStep: 3667, step-time 1.02, Acc: 0.1586, Loss: 5.5092, Perpxty: 246.95\n",
      "Epoch: 1, GlobalStep: 3717, step-time 1.03, Acc: 0.1931, Loss: 5.2203, Perpxty: 185.00\n",
      "Time taken to save checkpoint:  3.499197006225586\n",
      "Epoch: 1, GlobalStep: 3767, step-time 1.00, Acc: 0.1310, Loss: 5.4477, Perpxty: 232.23\n",
      "Epoch: 1, GlobalStep: 3817, step-time 1.05, Acc: 0.1556, Loss: 5.7401, Perpxty: 311.09\n",
      "Epoch: 1, GlobalStep: 3867, step-time 1.11, Acc: 0.1038, Loss: 6.0142, Perpxty: 409.18\n",
      "Epoch: 1, GlobalStep: 3917, step-time 1.06, Acc: 0.1511, Loss: 5.7938, Perpxty: 328.25\n",
      "Epoch: 1, GlobalStep: 3967, step-time 1.11, Acc: 0.1467, Loss: 5.3467, Perpxty: 209.92\n",
      "Epoch: 1, GlobalStep: 4017, step-time 1.12, Acc: 0.1259, Loss: 5.7976, Perpxty: 329.51\n",
      "Epoch: 1, GlobalStep: 4067, step-time 1.12, Acc: 0.1702, Loss: 5.5736, Perpxty: 263.37\n",
      "Epoch: 1, GlobalStep: 4117, step-time 1.03, Acc: 0.1156, Loss: 6.1260, Perpxty: 457.61\n",
      "Epoch: 1, GlobalStep: 4167, step-time 1.06, Acc: 0.1619, Loss: 5.4822, Perpxty: 240.37\n",
      "Epoch: 1, GlobalStep: 4217, step-time 1.14, Acc: 0.1250, Loss: 5.7716, Perpxty: 321.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, GlobalStep: 4267, step-time 1.14, Acc: 0.2000, Loss: 5.2561, Perpxty: 191.74\n",
      "Epoch: 1, GlobalStep: 4317, step-time 1.11, Acc: 0.2138, Loss: 5.2227, Perpxty: 185.44\n",
      "Epoch: 1, GlobalStep: 4367, step-time 1.03, Acc: 0.1600, Loss: 5.7159, Perpxty: 303.66\n",
      "Epoch: 1, GlobalStep: 4417, step-time 1.12, Acc: 0.2222, Loss: 5.3042, Perpxty: 201.18\n",
      "Epoch: 1, GlobalStep: 4467, step-time 1.12, Acc: 0.1714, Loss: 5.4314, Perpxty: 228.48\n",
      "Time taken to save checkpoint:  3.639807939529419\n",
      "Epoch: 1, GlobalStep: 4517, step-time 1.09, Acc: 0.1778, Loss: 5.2941, Perpxty: 199.16\n",
      "Epoch: 1, GlobalStep: 4567, step-time 1.00, Acc: 0.1440, Loss: 5.9217, Perpxty: 373.03\n",
      "Epoch: 1, GlobalStep: 4617, step-time 1.14, Acc: 0.2214, Loss: 5.1667, Perpxty: 175.34\n",
      "Epoch: 1, GlobalStep: 4667, step-time 1.12, Acc: 0.1786, Loss: 5.6152, Perpxty: 274.57\n",
      "Epoch: 1, GlobalStep: 4717, step-time 1.16, Acc: 0.2917, Loss: 4.4330, Perpxty: 84.18\n",
      "Epoch: 1, GlobalStep: 4767, step-time 1.09, Acc: 0.1643, Loss: 5.5643, Perpxty: 260.94\n",
      "Epoch: 1, GlobalStep: 4817, step-time 1.16, Acc: 0.1862, Loss: 4.9947, Perpxty: 147.62\n",
      "Epoch: 1, GlobalStep: 4867, step-time 1.11, Acc: 0.1440, Loss: 5.5191, Perpxty: 249.41\n",
      "Epoch: 1, GlobalStep: 4917, step-time 1.19, Acc: 0.1360, Loss: 5.6761, Perpxty: 291.80\n",
      "Epoch: 1, GlobalStep: 4967, step-time 1.22, Acc: 0.1925, Loss: 5.5507, Perpxty: 257.42\n",
      "Epoch: 1, GlobalStep: 5017, step-time 1.22, Acc: 0.1872, Loss: 5.4215, Perpxty: 226.21\n",
      "Epoch: 1, GlobalStep: 5067, step-time 1.20, Acc: 0.1571, Loss: 5.7237, Perpxty: 306.02\n",
      "Epoch: 1, GlobalStep: 5117, step-time 1.22, Acc: 0.1434, Loss: 5.8483, Perpxty: 346.63\n",
      "Epoch: 1, GlobalStep: 5167, step-time 1.22, Acc: 0.1795, Loss: 4.8142, Perpxty: 123.25\n",
      "Epoch: 1, GlobalStep: 5217, step-time 1.28, Acc: 0.1867, Loss: 5.2343, Perpxty: 187.60\n",
      "Time taken to save checkpoint:  3.2648582458496094\n",
      "Epoch: 1, GlobalStep: 5267, step-time 1.25, Acc: 0.1294, Loss: 5.5759, Perpxty: 263.99\n",
      "Epoch: 1, GlobalStep: 5317, step-time 1.30, Acc: 0.1103, Loss: 5.4190, Perpxty: 225.64\n",
      "Epoch: 1, GlobalStep: 5367, step-time 1.14, Acc: 0.1571, Loss: 5.3188, Perpxty: 204.15\n",
      "Epoch: 1, GlobalStep: 5417, step-time 1.12, Acc: 0.1750, Loss: 5.3978, Perpxty: 220.93\n",
      "Epoch: 1, GlobalStep: 5467, step-time 1.37, Acc: 0.1065, Loss: 5.9390, Perpxty: 379.54\n",
      "Epoch: 1, GlobalStep: 5517, step-time 1.16, Acc: 0.1429, Loss: 5.1290, Perpxty: 168.84\n",
      "Epoch: 1, GlobalStep: 5567, step-time 1.30, Acc: 0.2143, Loss: 5.2276, Perpxty: 186.34\n",
      "Epoch: 1, GlobalStep: 5617, step-time 1.28, Acc: 0.2000, Loss: 4.9520, Perpxty: 141.46\n",
      "Epoch: 1, GlobalStep: 5667, step-time 1.41, Acc: 0.1176, Loss: 5.8203, Perpxty: 337.08\n",
      "Epoch: 1, GlobalStep: 5717, step-time 1.34, Acc: 0.1702, Loss: 5.1226, Perpxty: 167.77\n",
      "Epoch: 1, GlobalStep: 5767, step-time 1.39, Acc: 0.1964, Loss: 5.0518, Perpxty: 156.30\n",
      "Epoch: 1, GlobalStep: 5817, step-time 1.69, Acc: 0.1413, Loss: 5.7396, Perpxty: 310.93\n",
      "Epoch: 1, GlobalStep: 5867, step-time 1.81, Acc: 0.1032, Loss: 5.7340, Perpxty: 309.22\n",
      "Epoch: 1, GlobalStep: 5917, step-time 1.80, Acc: 0.2222, Loss: 5.1313, Perpxty: 169.24\n",
      "Epoch: 1, GlobalStep: 5967, step-time 2.09, Acc: 0.1000, Loss: 5.7871, Perpxty: 326.08\n",
      "Time taken to save checkpoint:  3.5460832118988037\n",
      "Epoch: 1, GlobalStep: 6017, step-time 2.56, Acc: 0.1129, Loss: 5.4577, Perpxty: 234.56\n",
      "at the end of epoch: 1\n",
      "Average train loss = 5.46521993, Average perplexity = 236.32782459\n",
      "Average train acc = 0.16492239\n",
      "validation loss = 5.64187973, perplexity = 281.99228966\n",
      "Average Validation acc = 0.12549317\n",
      "Time taken to save checkpoint:  3.6211583614349365\n",
      "Time taken to save checkpoint:  3.46795392036438\n",
      "Epoch: 2, GlobalStep: 6033, step-time 0.59, Acc: 0.1154, Loss: 5.3387, Perpxty: 208.24\n",
      "Epoch: 2, GlobalStep: 6083, step-time 0.83, Acc: 0.2000, Loss: 5.2467, Perpxty: 189.93\n",
      "Epoch: 2, GlobalStep: 6133, step-time 0.81, Acc: 0.2296, Loss: 5.1685, Perpxty: 175.65\n",
      "Epoch: 2, GlobalStep: 6183, step-time 1.08, Acc: 0.1424, Loss: 5.9221, Perpxty: 373.21\n",
      "Epoch: 2, GlobalStep: 6233, step-time 0.97, Acc: 0.1793, Loss: 5.1761, Perpxty: 176.99\n",
      "Epoch: 2, GlobalStep: 6283, step-time 1.00, Acc: 0.2000, Loss: 5.5894, Perpxty: 267.57\n",
      "Epoch: 2, GlobalStep: 6333, step-time 0.94, Acc: 0.2348, Loss: 4.9856, Perpxty: 146.29\n",
      "Epoch: 2, GlobalStep: 6383, step-time 0.94, Acc: 0.1840, Loss: 5.2424, Perpxty: 189.11\n",
      "Epoch: 2, GlobalStep: 6433, step-time 1.02, Acc: 0.1923, Loss: 5.5186, Perpxty: 249.29\n",
      "Epoch: 2, GlobalStep: 6483, step-time 0.92, Acc: 0.1833, Loss: 5.4386, Perpxty: 230.13\n",
      "Epoch: 2, GlobalStep: 6533, step-time 0.95, Acc: 0.2462, Loss: 4.9241, Perpxty: 137.57\n",
      "Epoch: 2, GlobalStep: 6583, step-time 0.94, Acc: 0.1920, Loss: 5.0533, Perpxty: 156.55\n",
      "Epoch: 2, GlobalStep: 6633, step-time 0.95, Acc: 0.1636, Loss: 5.2394, Perpxty: 188.56\n",
      "Epoch: 2, GlobalStep: 6683, step-time 1.02, Acc: 0.2138, Loss: 5.2772, Perpxty: 195.83\n",
      "Epoch: 2, GlobalStep: 6733, step-time 1.02, Acc: 0.2069, Loss: 5.0477, Perpxty: 155.67\n",
      "Time taken to save checkpoint:  3.342991828918457\n",
      "Epoch: 2, GlobalStep: 6783, step-time 0.97, Acc: 0.1448, Loss: 5.2858, Perpxty: 197.51\n",
      "Epoch: 2, GlobalStep: 6833, step-time 1.08, Acc: 0.1689, Loss: 5.5687, Perpxty: 262.09\n",
      "Epoch: 2, GlobalStep: 6883, step-time 1.09, Acc: 0.1192, Loss: 5.8620, Perpxty: 351.44\n",
      "Epoch: 2, GlobalStep: 6933, step-time 1.06, Acc: 0.1511, Loss: 5.6097, Perpxty: 273.07\n",
      "Epoch: 2, GlobalStep: 6983, step-time 1.06, Acc: 0.1600, Loss: 4.9789, Perpxty: 145.31\n",
      "Epoch: 2, GlobalStep: 7033, step-time 1.16, Acc: 0.1222, Loss: 5.5802, Perpxty: 265.13\n",
      "Epoch: 2, GlobalStep: 7083, step-time 1.11, Acc: 0.1830, Loss: 5.4393, Perpxty: 230.28\n",
      "Epoch: 2, GlobalStep: 7133, step-time 1.06, Acc: 0.1111, Loss: 5.9241, Perpxty: 373.94\n",
      "Epoch: 2, GlobalStep: 7183, step-time 1.03, Acc: 0.1714, Loss: 5.3289, Perpxty: 206.21\n",
      "Epoch: 2, GlobalStep: 7233, step-time 1.16, Acc: 0.1143, Loss: 5.7433, Perpxty: 312.10\n",
      "Epoch: 2, GlobalStep: 7283, step-time 1.03, Acc: 0.2077, Loss: 5.0615, Perpxty: 157.82\n",
      "Epoch: 2, GlobalStep: 7333, step-time 1.14, Acc: 0.2276, Loss: 5.0964, Perpxty: 163.43\n",
      "Epoch: 2, GlobalStep: 7383, step-time 1.03, Acc: 0.1440, Loss: 5.6273, Perpxty: 277.91\n",
      "Epoch: 2, GlobalStep: 7433, step-time 1.11, Acc: 0.2296, Loss: 5.2052, Perpxty: 182.23\n",
      "Epoch: 2, GlobalStep: 7483, step-time 1.06, Acc: 0.1643, Loss: 5.2515, Perpxty: 190.86\n",
      "Time taken to save checkpoint:  3.5696017742156982\n",
      "Epoch: 2, GlobalStep: 7533, step-time 1.08, Acc: 0.1852, Loss: 5.1141, Perpxty: 166.35\n",
      "Epoch: 2, GlobalStep: 7583, step-time 1.00, Acc: 0.1760, Loss: 5.6379, Perpxty: 280.86\n",
      "Epoch: 2, GlobalStep: 7633, step-time 1.11, Acc: 0.2286, Loss: 5.0244, Perpxty: 152.09\n",
      "Epoch: 2, GlobalStep: 7683, step-time 1.14, Acc: 0.2000, Loss: 5.5519, Perpxty: 257.72\n",
      "Epoch: 2, GlobalStep: 7733, step-time 1.16, Acc: 0.3000, Loss: 4.1699, Perpxty: 64.71\n",
      "Epoch: 2, GlobalStep: 7783, step-time 1.14, Acc: 0.1714, Loss: 5.4160, Perpxty: 224.97\n",
      "Epoch: 2, GlobalStep: 7833, step-time 1.16, Acc: 0.2345, Loss: 4.6767, Perpxty: 107.42\n",
      "Epoch: 2, GlobalStep: 7883, step-time 1.14, Acc: 0.1760, Loss: 5.2874, Perpxty: 197.82\n",
      "Epoch: 2, GlobalStep: 7933, step-time 1.20, Acc: 0.1480, Loss: 5.4173, Perpxty: 225.27\n",
      "Epoch: 2, GlobalStep: 7983, step-time 1.22, Acc: 0.1925, Loss: 5.2988, Perpxty: 200.10\n",
      "Epoch: 2, GlobalStep: 8033, step-time 1.30, Acc: 0.1872, Loss: 5.2410, Perpxty: 188.86\n",
      "Epoch: 2, GlobalStep: 8083, step-time 1.23, Acc: 0.1429, Loss: 5.5493, Perpxty: 257.06\n",
      "Epoch: 2, GlobalStep: 8133, step-time 1.20, Acc: 0.1321, Loss: 5.6822, Perpxty: 293.59\n",
      "Epoch: 2, GlobalStep: 8183, step-time 1.17, Acc: 0.1795, Loss: 4.6522, Perpxty: 104.81\n",
      "Epoch: 2, GlobalStep: 8233, step-time 1.23, Acc: 0.1778, Loss: 5.0558, Perpxty: 156.93\n",
      "Time taken to save checkpoint:  3.5616941452026367\n",
      "Epoch: 2, GlobalStep: 8283, step-time 1.17, Acc: 0.1451, Loss: 5.3496, Perpxty: 210.52\n",
      "Epoch: 2, GlobalStep: 8333, step-time 1.27, Acc: 0.1069, Loss: 5.2510, Perpxty: 190.77\n",
      "Epoch: 2, GlobalStep: 8383, step-time 1.17, Acc: 0.1762, Loss: 5.1589, Perpxty: 173.97\n",
      "Epoch: 2, GlobalStep: 8433, step-time 1.16, Acc: 0.1750, Loss: 5.2964, Perpxty: 199.61\n",
      "Epoch: 2, GlobalStep: 8483, step-time 1.47, Acc: 0.1247, Loss: 5.6803, Perpxty: 293.05\n",
      "Epoch: 2, GlobalStep: 8533, step-time 1.11, Acc: 0.1786, Loss: 4.8415, Perpxty: 126.66\n",
      "Epoch: 2, GlobalStep: 8583, step-time 1.23, Acc: 0.2357, Loss: 4.9765, Perpxty: 144.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, GlobalStep: 8633, step-time 1.31, Acc: 0.1714, Loss: 4.7756, Perpxty: 118.59\n",
      "Epoch: 2, GlobalStep: 8683, step-time 1.45, Acc: 0.1255, Loss: 5.7033, Perpxty: 299.84\n",
      "Epoch: 2, GlobalStep: 8733, step-time 1.33, Acc: 0.1702, Loss: 4.9526, Perpxty: 141.54\n",
      "Epoch: 2, GlobalStep: 8783, step-time 1.37, Acc: 0.1891, Loss: 4.8946, Perpxty: 133.56\n",
      "Epoch: 2, GlobalStep: 8833, step-time 1.66, Acc: 0.1326, Loss: 5.6576, Perpxty: 286.45\n",
      "Epoch: 2, GlobalStep: 8883, step-time 1.83, Acc: 0.1054, Loss: 5.5768, Perpxty: 264.23\n",
      "Epoch: 2, GlobalStep: 8933, step-time 1.81, Acc: 0.1956, Loss: 4.9340, Perpxty: 138.93\n",
      "Epoch: 2, GlobalStep: 8983, step-time 2.16, Acc: 0.0957, Loss: 5.6462, Perpxty: 283.22\n",
      "Time taken to save checkpoint:  3.483555793762207\n",
      "Epoch: 2, GlobalStep: 9033, step-time 2.52, Acc: 0.1294, Loss: 5.3115, Perpxty: 202.65\n",
      "at the end of epoch: 2\n",
      "Average train loss = 5.27297594, Average perplexity = 194.99539451\n",
      "Average train acc = 0.17369296\n",
      "validation loss = 5.54251548, perplexity = 255.31944359\n",
      "Average Validation acc = 0.13553492\n",
      "Time taken to save checkpoint:  3.571141004562378\n",
      "Time taken to save checkpoint:  4.124030113220215\n",
      "Epoch: 3, GlobalStep: 9049, step-time 0.64, Acc: 0.1385, Loss: 5.2419, Perpxty: 189.03\n",
      "Epoch: 3, GlobalStep: 9099, step-time 0.81, Acc: 0.2071, Loss: 4.9022, Perpxty: 134.59\n",
      "Epoch: 3, GlobalStep: 9149, step-time 0.80, Acc: 0.1556, Loss: 5.0546, Perpxty: 156.74\n",
      "Epoch: 3, GlobalStep: 9199, step-time 1.03, Acc: 0.1085, Loss: 5.8931, Perpxty: 362.53\n",
      "Epoch: 3, GlobalStep: 9249, step-time 0.97, Acc: 0.1931, Loss: 5.0179, Perpxty: 151.09\n",
      "Epoch: 3, GlobalStep: 9299, step-time 1.00, Acc: 0.2000, Loss: 5.4202, Perpxty: 225.92\n",
      "Epoch: 3, GlobalStep: 9349, step-time 0.92, Acc: 0.2174, Loss: 4.8444, Perpxty: 127.03\n",
      "Epoch: 3, GlobalStep: 9399, step-time 0.92, Acc: 0.1760, Loss: 5.1183, Perpxty: 167.05\n",
      "Epoch: 3, GlobalStep: 9449, step-time 0.97, Acc: 0.2308, Loss: 5.3281, Perpxty: 206.05\n",
      "Epoch: 3, GlobalStep: 9499, step-time 0.98, Acc: 0.1917, Loss: 5.1174, Perpxty: 166.90\n",
      "Epoch: 3, GlobalStep: 9549, step-time 0.95, Acc: 0.2385, Loss: 4.8137, Perpxty: 123.19\n",
      "Epoch: 3, GlobalStep: 9599, step-time 0.97, Acc: 0.2000, Loss: 4.8128, Perpxty: 123.08\n",
      "Epoch: 3, GlobalStep: 9649, step-time 1.00, Acc: 0.2182, Loss: 4.8984, Perpxty: 134.08\n",
      "Epoch: 3, GlobalStep: 9699, step-time 1.02, Acc: 0.2276, Loss: 5.0235, Perpxty: 151.95\n",
      "Epoch: 3, GlobalStep: 9749, step-time 0.97, Acc: 0.2069, Loss: 4.9230, Perpxty: 137.41\n",
      "Time taken to save checkpoint:  3.561652898788452\n",
      "Epoch: 3, GlobalStep: 9799, step-time 1.03, Acc: 0.1517, Loss: 5.1661, Perpxty: 175.22\n",
      "Epoch: 3, GlobalStep: 9849, step-time 1.05, Acc: 0.1733, Loss: 5.3758, Perpxty: 216.12\n",
      "Epoch: 3, GlobalStep: 9899, step-time 1.09, Acc: 0.1346, Loss: 5.6675, Perpxty: 289.32\n",
      "Epoch: 3, GlobalStep: 9949, step-time 1.09, Acc: 0.1511, Loss: 5.4682, Perpxty: 237.04\n",
      "Epoch: 3, GlobalStep: 9999, step-time 1.05, Acc: 0.1556, Loss: 4.8361, Perpxty: 125.97\n",
      "Epoch: 3, GlobalStep: 10049, step-time 1.12, Acc: 0.1259, Loss: 5.4692, Perpxty: 237.28\n",
      "Epoch: 3, GlobalStep: 10099, step-time 1.05, Acc: 0.1830, Loss: 5.3123, Perpxty: 202.82\n",
      "Epoch: 3, GlobalStep: 10149, step-time 1.11, Acc: 0.1644, Loss: 5.7644, Perpxty: 318.76\n",
      "Epoch: 3, GlobalStep: 10199, step-time 1.05, Acc: 0.1762, Loss: 5.1336, Perpxty: 169.63\n",
      "Epoch: 3, GlobalStep: 10249, step-time 1.17, Acc: 0.1107, Loss: 5.5993, Perpxty: 270.24\n",
      "Epoch: 3, GlobalStep: 10299, step-time 1.05, Acc: 0.2308, Loss: 4.8595, Perpxty: 128.96\n",
      "Epoch: 3, GlobalStep: 10349, step-time 1.16, Acc: 0.2069, Loss: 4.9202, Perpxty: 137.03\n",
      "Epoch: 3, GlobalStep: 10399, step-time 1.05, Acc: 0.1680, Loss: 5.4051, Perpxty: 222.55\n",
      "Epoch: 3, GlobalStep: 10449, step-time 1.12, Acc: 0.2148, Loss: 4.9872, Perpxty: 146.52\n",
      "Epoch: 3, GlobalStep: 10499, step-time 1.14, Acc: 0.1929, Loss: 5.0697, Perpxty: 159.13\n",
      "Time taken to save checkpoint:  3.7803492546081543\n",
      "Epoch: 3, GlobalStep: 10549, step-time 1.11, Acc: 0.1630, Loss: 4.9528, Perpxty: 141.58\n",
      "Epoch: 3, GlobalStep: 10599, step-time 1.02, Acc: 0.2160, Loss: 5.5110, Perpxty: 247.40\n",
      "Epoch: 3, GlobalStep: 10649, step-time 1.06, Acc: 0.2429, Loss: 4.7492, Perpxty: 115.49\n",
      "Epoch: 3, GlobalStep: 10699, step-time 1.12, Acc: 0.1714, Loss: 5.4082, Perpxty: 223.24\n",
      "Epoch: 3, GlobalStep: 10749, step-time 1.14, Acc: 0.3083, Loss: 4.0530, Perpxty: 57.57\n",
      "Epoch: 3, GlobalStep: 10799, step-time 1.09, Acc: 0.1643, Loss: 5.1689, Perpxty: 175.71\n",
      "Epoch: 3, GlobalStep: 10849, step-time 1.09, Acc: 0.2276, Loss: 4.5704, Perpxty: 96.58\n",
      "Epoch: 3, GlobalStep: 10899, step-time 1.06, Acc: 0.1920, Loss: 5.1578, Perpxty: 173.78\n",
      "Epoch: 3, GlobalStep: 10949, step-time 1.19, Acc: 0.1320, Loss: 5.2218, Perpxty: 185.26\n",
      "Epoch: 3, GlobalStep: 10999, step-time 1.31, Acc: 0.2075, Loss: 5.1652, Perpxty: 175.08\n",
      "Epoch: 3, GlobalStep: 11049, step-time 1.23, Acc: 0.1787, Loss: 5.1162, Perpxty: 166.71\n",
      "Epoch: 3, GlobalStep: 11099, step-time 1.25, Acc: 0.1500, Loss: 5.4500, Perpxty: 232.75\n",
      "Epoch: 3, GlobalStep: 11149, step-time 1.23, Acc: 0.1283, Loss: 5.4855, Perpxty: 241.17\n",
      "Epoch: 3, GlobalStep: 11199, step-time 1.19, Acc: 0.2000, Loss: 4.4555, Perpxty: 86.10\n",
      "Epoch: 3, GlobalStep: 11249, step-time 1.34, Acc: 0.1911, Loss: 4.8809, Perpxty: 131.75\n",
      "Time taken to save checkpoint:  3.4054856300354004\n",
      "Epoch: 3, GlobalStep: 11299, step-time 1.22, Acc: 0.1490, Loss: 5.2306, Perpxty: 186.90\n",
      "Epoch: 3, GlobalStep: 11349, step-time 1.28, Acc: 0.1138, Loss: 5.1470, Perpxty: 171.91\n",
      "Epoch: 3, GlobalStep: 11399, step-time 1.19, Acc: 0.1762, Loss: 5.0516, Perpxty: 156.28\n",
      "Epoch: 3, GlobalStep: 11449, step-time 1.12, Acc: 0.1800, Loss: 5.1879, Perpxty: 179.09\n",
      "Epoch: 3, GlobalStep: 11499, step-time 1.41, Acc: 0.1273, Loss: 5.5442, Perpxty: 255.74\n",
      "Epoch: 3, GlobalStep: 11549, step-time 1.16, Acc: 0.2214, Loss: 4.6328, Perpxty: 102.80\n",
      "Epoch: 3, GlobalStep: 11599, step-time 1.25, Acc: 0.2214, Loss: 4.9623, Perpxty: 142.92\n",
      "Epoch: 3, GlobalStep: 11649, step-time 1.28, Acc: 0.1857, Loss: 4.5834, Perpxty: 97.84\n",
      "Epoch: 3, GlobalStep: 11699, step-time 1.41, Acc: 0.1373, Loss: 5.4675, Perpxty: 236.86\n",
      "Epoch: 3, GlobalStep: 11749, step-time 1.37, Acc: 0.2128, Loss: 4.7609, Perpxty: 116.85\n",
      "Epoch: 3, GlobalStep: 11799, step-time 1.37, Acc: 0.2255, Loss: 4.7703, Perpxty: 117.96\n",
      "Epoch: 3, GlobalStep: 11849, step-time 1.67, Acc: 0.1478, Loss: 5.5340, Perpxty: 253.15\n",
      "Epoch: 3, GlobalStep: 11899, step-time 1.81, Acc: 0.1204, Loss: 5.4910, Perpxty: 242.51\n",
      "Epoch: 3, GlobalStep: 11949, step-time 1.77, Acc: 0.1956, Loss: 4.7835, Perpxty: 119.52\n",
      "Epoch: 3, GlobalStep: 11999, step-time 2.11, Acc: 0.1213, Loss: 5.5033, Perpxty: 245.49\n",
      "Time taken to save checkpoint:  3.4054887294769287\n",
      "Epoch: 3, GlobalStep: 12049, step-time 2.59, Acc: 0.1247, Loss: 5.2224, Perpxty: 185.38\n",
      "at the end of epoch: 3\n",
      "Average train loss = 5.11973061, Average perplexity = 167.29029757\n",
      "Average train acc = 0.17935418\n",
      "validation loss = 5.47712744, perplexity = 239.15872230\n",
      "Average Validation acc = 0.13523020\n",
      "Time taken to save checkpoint:  3.664034605026245\n",
      "Time taken to save checkpoint:  3.6085546016693115\n",
      "Epoch: 4, GlobalStep: 12065, step-time 0.57, Acc: 0.1385, Loss: 5.1077, Perpxty: 165.28\n",
      "Epoch: 4, GlobalStep: 12115, step-time 0.81, Acc: 0.1929, Loss: 4.7385, Perpxty: 114.27\n",
      "Epoch: 4, GlobalStep: 12165, step-time 0.80, Acc: 0.1778, Loss: 4.9383, Perpxty: 139.53\n",
      "Epoch: 4, GlobalStep: 12215, step-time 1.02, Acc: 0.1288, Loss: 5.6355, Perpxty: 280.19\n",
      "Epoch: 4, GlobalStep: 12265, step-time 0.97, Acc: 0.1724, Loss: 4.9084, Perpxty: 135.42\n",
      "Epoch: 4, GlobalStep: 12315, step-time 1.00, Acc: 0.2069, Loss: 5.3080, Perpxty: 201.94\n",
      "Epoch: 4, GlobalStep: 12365, step-time 0.94, Acc: 0.2261, Loss: 4.6511, Perpxty: 104.70\n",
      "Epoch: 4, GlobalStep: 12415, step-time 0.94, Acc: 0.2080, Loss: 4.8302, Perpxty: 125.23\n",
      "Epoch: 4, GlobalStep: 12465, step-time 0.97, Acc: 0.2538, Loss: 5.0693, Perpxty: 159.06\n",
      "Epoch: 4, GlobalStep: 12515, step-time 0.97, Acc: 0.2083, Loss: 4.9789, Perpxty: 145.31\n",
      "Epoch: 4, GlobalStep: 12565, step-time 0.95, Acc: 0.2154, Loss: 4.6855, Perpxty: 108.37\n",
      "Epoch: 4, GlobalStep: 12615, step-time 0.94, Acc: 0.2320, Loss: 4.8045, Perpxty: 122.06\n",
      "Epoch: 4, GlobalStep: 12665, step-time 0.92, Acc: 0.2273, Loss: 4.7917, Perpxty: 120.51\n",
      "Epoch: 4, GlobalStep: 12715, step-time 0.95, Acc: 0.2207, Loss: 4.8774, Perpxty: 131.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, GlobalStep: 12765, step-time 1.03, Acc: 0.1931, Loss: 4.8818, Perpxty: 131.87\n",
      "Time taken to save checkpoint:  3.5304408073425293\n",
      "Epoch: 4, GlobalStep: 12815, step-time 1.08, Acc: 0.1241, Loss: 5.1952, Perpxty: 180.40\n",
      "Epoch: 4, GlobalStep: 12865, step-time 1.08, Acc: 0.1867, Loss: 5.2036, Perpxty: 181.92\n",
      "Epoch: 4, GlobalStep: 12915, step-time 1.09, Acc: 0.1346, Loss: 5.5062, Perpxty: 246.22\n",
      "Epoch: 4, GlobalStep: 12965, step-time 1.09, Acc: 0.1422, Loss: 5.3526, Perpxty: 211.15\n",
      "Epoch: 4, GlobalStep: 13015, step-time 1.08, Acc: 0.1689, Loss: 4.6939, Perpxty: 109.28\n",
      "Epoch: 4, GlobalStep: 13065, step-time 1.14, Acc: 0.1259, Loss: 5.3540, Perpxty: 211.46\n",
      "Epoch: 4, GlobalStep: 13115, step-time 1.09, Acc: 0.1702, Loss: 5.2171, Perpxty: 184.40\n",
      "Epoch: 4, GlobalStep: 13165, step-time 1.03, Acc: 0.1333, Loss: 5.6639, Perpxty: 288.26\n",
      "Epoch: 4, GlobalStep: 13215, step-time 1.02, Acc: 0.1619, Loss: 5.1308, Perpxty: 169.15\n",
      "Epoch: 4, GlobalStep: 13265, step-time 1.12, Acc: 0.1107, Loss: 5.4478, Perpxty: 232.24\n",
      "Epoch: 4, GlobalStep: 13315, step-time 1.08, Acc: 0.2231, Loss: 4.6820, Perpxty: 107.98\n",
      "Epoch: 4, GlobalStep: 13365, step-time 1.16, Acc: 0.2138, Loss: 4.8367, Perpxty: 126.05\n",
      "Epoch: 4, GlobalStep: 13415, step-time 1.09, Acc: 0.1440, Loss: 5.3505, Perpxty: 210.70\n",
      "Epoch: 4, GlobalStep: 13465, step-time 1.14, Acc: 0.2370, Loss: 4.8507, Perpxty: 127.83\n",
      "Epoch: 4, GlobalStep: 13515, step-time 1.11, Acc: 0.1714, Loss: 5.0151, Perpxty: 150.67\n",
      "Time taken to save checkpoint:  3.5929250717163086\n",
      "Epoch: 4, GlobalStep: 13565, step-time 1.11, Acc: 0.2074, Loss: 4.8937, Perpxty: 133.44\n",
      "Epoch: 4, GlobalStep: 13615, step-time 1.00, Acc: 0.2080, Loss: 5.3907, Perpxty: 219.37\n",
      "Epoch: 4, GlobalStep: 13665, step-time 1.11, Acc: 0.2214, Loss: 4.6750, Perpxty: 107.23\n",
      "Epoch: 4, GlobalStep: 13715, step-time 1.09, Acc: 0.1643, Loss: 5.3473, Perpxty: 210.05\n",
      "Epoch: 4, GlobalStep: 13765, step-time 1.12, Acc: 0.3000, Loss: 3.8751, Perpxty: 48.19\n",
      "Epoch: 4, GlobalStep: 13815, step-time 1.11, Acc: 0.1571, Loss: 5.0170, Perpxty: 150.96\n",
      "Epoch: 4, GlobalStep: 13865, step-time 1.09, Acc: 0.2552, Loss: 4.3905, Perpxty: 80.68\n",
      "Epoch: 4, GlobalStep: 13915, step-time 1.03, Acc: 0.1680, Loss: 4.9998, Perpxty: 148.39\n",
      "Epoch: 4, GlobalStep: 13965, step-time 1.23, Acc: 0.1360, Loss: 5.1109, Perpxty: 165.82\n",
      "Epoch: 4, GlobalStep: 14015, step-time 1.25, Acc: 0.2264, Loss: 4.9883, Perpxty: 146.68\n",
      "Epoch: 4, GlobalStep: 14065, step-time 1.25, Acc: 0.1660, Loss: 4.9659, Perpxty: 143.43\n",
      "Epoch: 4, GlobalStep: 14115, step-time 1.23, Acc: 0.1643, Loss: 5.3122, Perpxty: 202.80\n",
      "Epoch: 4, GlobalStep: 14165, step-time 1.27, Acc: 0.1472, Loss: 5.3206, Perpxty: 204.50\n",
      "Epoch: 4, GlobalStep: 14215, step-time 1.22, Acc: 0.2205, Loss: 4.4146, Perpxty: 82.65\n",
      "Epoch: 4, GlobalStep: 14265, step-time 1.20, Acc: 0.2267, Loss: 4.7036, Perpxty: 110.34\n",
      "Time taken to save checkpoint:  3.452362060546875\n",
      "Epoch: 4, GlobalStep: 14315, step-time 1.20, Acc: 0.1451, Loss: 5.0868, Perpxty: 161.88\n",
      "Epoch: 4, GlobalStep: 14365, step-time 1.31, Acc: 0.1379, Loss: 4.9515, Perpxty: 141.38\n",
      "Epoch: 4, GlobalStep: 14415, step-time 1.22, Acc: 0.1667, Loss: 4.9537, Perpxty: 141.70\n",
      "Epoch: 4, GlobalStep: 14465, step-time 1.16, Acc: 0.1900, Loss: 4.9913, Perpxty: 147.13\n",
      "Epoch: 4, GlobalStep: 14515, step-time 1.37, Acc: 0.1403, Loss: 5.4214, Perpxty: 226.19\n",
      "Epoch: 4, GlobalStep: 14565, step-time 1.17, Acc: 0.1857, Loss: 4.5124, Perpxty: 91.14\n",
      "Epoch: 4, GlobalStep: 14615, step-time 1.22, Acc: 0.2071, Loss: 4.6443, Perpxty: 103.99\n",
      "Epoch: 4, GlobalStep: 14665, step-time 1.23, Acc: 0.1857, Loss: 4.4952, Perpxty: 89.59\n",
      "Epoch: 4, GlobalStep: 14715, step-time 1.37, Acc: 0.1176, Loss: 5.3931, Perpxty: 219.88\n",
      "Epoch: 4, GlobalStep: 14765, step-time 1.31, Acc: 0.2000, Loss: 4.5855, Perpxty: 98.05\n",
      "Epoch: 4, GlobalStep: 14815, step-time 1.37, Acc: 0.2036, Loss: 4.6619, Perpxty: 105.84\n",
      "Epoch: 4, GlobalStep: 14865, step-time 1.62, Acc: 0.1326, Loss: 5.4805, Perpxty: 239.96\n",
      "Epoch: 4, GlobalStep: 14915, step-time 1.80, Acc: 0.1075, Loss: 5.3174, Perpxty: 203.86\n",
      "Epoch: 4, GlobalStep: 14965, step-time 1.78, Acc: 0.2133, Loss: 4.6353, Perpxty: 103.06\n",
      "Epoch: 4, GlobalStep: 15015, step-time 2.16, Acc: 0.1043, Loss: 5.4655, Perpxty: 236.39\n",
      "Time taken to save checkpoint:  3.405478000640869\n",
      "Epoch: 4, GlobalStep: 15065, step-time 2.47, Acc: 0.1412, Loss: 5.0589, Perpxty: 157.42\n",
      "at the end of epoch: 4\n",
      "Average train loss = 4.99193803, Average perplexity = 147.22146666\n",
      "Average train acc = 0.18340272\n",
      "validation loss = 5.38946545, perplexity = 219.08624104\n",
      "Average Validation acc = 0.14136403\n",
      "Time taken to save checkpoint:  3.4815144538879395\n",
      "Time taken to save checkpoint:  3.49920654296875\n",
      "Epoch: 5, GlobalStep: 15081, step-time 0.59, Acc: 0.1231, Loss: 4.9173, Perpxty: 136.63\n",
      "Epoch: 5, GlobalStep: 15131, step-time 0.83, Acc: 0.2214, Loss: 4.5393, Perpxty: 93.62\n",
      "Epoch: 5, GlobalStep: 15181, step-time 0.80, Acc: 0.2000, Loss: 4.8299, Perpxty: 125.20\n",
      "Epoch: 5, GlobalStep: 15231, step-time 1.03, Acc: 0.1288, Loss: 5.5833, Perpxty: 265.96\n",
      "Epoch: 5, GlobalStep: 15281, step-time 0.97, Acc: 0.1862, Loss: 4.8113, Perpxty: 122.89\n",
      "Epoch: 5, GlobalStep: 15331, step-time 0.98, Acc: 0.2138, Loss: 5.1679, Perpxty: 175.55\n",
      "Epoch: 5, GlobalStep: 15381, step-time 0.92, Acc: 0.2435, Loss: 4.4902, Perpxty: 89.14\n",
      "Epoch: 5, GlobalStep: 15431, step-time 0.97, Acc: 0.2480, Loss: 4.7012, Perpxty: 110.08\n",
      "Epoch: 5, GlobalStep: 15481, step-time 0.97, Acc: 0.2000, Loss: 4.9541, Perpxty: 141.75\n",
      "Epoch: 5, GlobalStep: 15531, step-time 0.98, Acc: 0.1500, Loss: 4.8869, Perpxty: 132.54\n",
      "Epoch: 5, GlobalStep: 15581, step-time 0.95, Acc: 0.2154, Loss: 4.5966, Perpxty: 99.14\n",
      "Epoch: 5, GlobalStep: 15631, step-time 0.94, Acc: 0.2000, Loss: 4.5748, Perpxty: 97.01\n",
      "Epoch: 5, GlobalStep: 15681, step-time 0.98, Acc: 0.2182, Loss: 4.6706, Perpxty: 106.76\n",
      "Epoch: 5, GlobalStep: 15731, step-time 0.97, Acc: 0.2276, Loss: 4.7297, Perpxty: 113.27\n",
      "Epoch: 5, GlobalStep: 15781, step-time 0.97, Acc: 0.2138, Loss: 4.7981, Perpxty: 121.28\n",
      "Time taken to save checkpoint:  3.5304388999938965\n",
      "Epoch: 5, GlobalStep: 15831, step-time 1.00, Acc: 0.1517, Loss: 5.0099, Perpxty: 149.89\n",
      "Epoch: 5, GlobalStep: 15881, step-time 1.06, Acc: 0.1911, Loss: 5.0351, Perpxty: 153.71\n",
      "Epoch: 5, GlobalStep: 15931, step-time 1.12, Acc: 0.1346, Loss: 5.4429, Perpxty: 231.12\n",
      "Epoch: 5, GlobalStep: 15981, step-time 1.06, Acc: 0.1511, Loss: 5.2382, Perpxty: 188.33\n",
      "Epoch: 5, GlobalStep: 16031, step-time 1.11, Acc: 0.1778, Loss: 4.5844, Perpxty: 97.94\n",
      "Epoch: 5, GlobalStep: 16081, step-time 1.12, Acc: 0.1259, Loss: 5.3330, Perpxty: 207.06\n",
      "Epoch: 5, GlobalStep: 16131, step-time 1.08, Acc: 0.1660, Loss: 5.1290, Perpxty: 168.85\n",
      "Epoch: 5, GlobalStep: 16181, step-time 1.03, Acc: 0.1600, Loss: 5.4475, Perpxty: 232.17\n",
      "Epoch: 5, GlobalStep: 16231, step-time 1.06, Acc: 0.1714, Loss: 5.0084, Perpxty: 149.66\n",
      "Epoch: 5, GlobalStep: 16281, step-time 1.14, Acc: 0.1107, Loss: 5.4057, Perpxty: 222.67\n",
      "Epoch: 5, GlobalStep: 16331, step-time 1.03, Acc: 0.2154, Loss: 4.4385, Perpxty: 84.65\n",
      "Epoch: 5, GlobalStep: 16381, step-time 1.19, Acc: 0.1724, Loss: 4.7089, Perpxty: 110.93\n",
      "Epoch: 5, GlobalStep: 16431, step-time 1.06, Acc: 0.1760, Loss: 5.2452, Perpxty: 189.66\n",
      "Epoch: 5, GlobalStep: 16481, step-time 1.11, Acc: 0.2222, Loss: 4.7440, Perpxty: 114.89\n",
      "Epoch: 5, GlobalStep: 16531, step-time 1.06, Acc: 0.2286, Loss: 4.7121, Perpxty: 111.28\n",
      "Time taken to save checkpoint:  3.460735559463501\n",
      "Epoch: 5, GlobalStep: 16581, step-time 1.16, Acc: 0.1704, Loss: 4.8640, Perpxty: 129.54\n",
      "Epoch: 5, GlobalStep: 16631, step-time 1.02, Acc: 0.1840, Loss: 5.3730, Perpxty: 215.52\n",
      "Epoch: 5, GlobalStep: 16681, step-time 1.12, Acc: 0.2357, Loss: 4.6214, Perpxty: 101.63\n",
      "Epoch: 5, GlobalStep: 16731, step-time 1.12, Acc: 0.1571, Loss: 5.2486, Perpxty: 190.29\n",
      "Epoch: 5, GlobalStep: 16781, step-time 1.12, Acc: 0.3250, Loss: 3.7634, Perpxty: 43.09\n",
      "Epoch: 5, GlobalStep: 16831, step-time 1.08, Acc: 0.2071, Loss: 4.8664, Perpxty: 129.86\n",
      "Epoch: 5, GlobalStep: 16881, step-time 1.16, Acc: 0.2276, Loss: 4.4521, Perpxty: 85.81\n",
      "Epoch: 5, GlobalStep: 16931, step-time 1.09, Acc: 0.1840, Loss: 4.8400, Perpxty: 126.47\n",
      "Epoch: 5, GlobalStep: 16981, step-time 1.19, Acc: 0.1400, Loss: 4.9607, Perpxty: 142.69\n",
      "Epoch: 5, GlobalStep: 17031, step-time 1.20, Acc: 0.2566, Loss: 4.8974, Perpxty: 133.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, GlobalStep: 17081, step-time 1.22, Acc: 0.1957, Loss: 4.9125, Perpxty: 135.97\n",
      "Epoch: 5, GlobalStep: 17131, step-time 1.28, Acc: 0.1607, Loss: 5.2678, Perpxty: 193.98\n",
      "Epoch: 5, GlobalStep: 17181, step-time 1.22, Acc: 0.1660, Loss: 5.1789, Perpxty: 177.49\n",
      "Epoch: 5, GlobalStep: 17231, step-time 1.19, Acc: 0.2154, Loss: 4.2761, Perpxty: 71.96\n",
      "Epoch: 5, GlobalStep: 17281, step-time 1.20, Acc: 0.1867, Loss: 4.6090, Perpxty: 100.39\n",
      "Time taken to save checkpoint:  3.3742470741271973\n",
      "Epoch: 5, GlobalStep: 17331, step-time 1.28, Acc: 0.1529, Loss: 4.9865, Perpxty: 146.43\n",
      "Epoch: 5, GlobalStep: 17381, step-time 1.27, Acc: 0.1552, Loss: 4.8055, Perpxty: 122.18\n",
      "Epoch: 5, GlobalStep: 17431, step-time 1.20, Acc: 0.1714, Loss: 4.8577, Perpxty: 128.72\n",
      "Epoch: 5, GlobalStep: 17481, step-time 1.11, Acc: 0.1850, Loss: 4.8831, Perpxty: 132.03\n",
      "Epoch: 5, GlobalStep: 17531, step-time 1.42, Acc: 0.1455, Loss: 5.2976, Perpxty: 199.85\n",
      "Epoch: 5, GlobalStep: 17581, step-time 1.16, Acc: 0.1929, Loss: 4.3859, Perpxty: 80.31\n",
      "Epoch: 5, GlobalStep: 17631, step-time 1.28, Acc: 0.2143, Loss: 4.5641, Perpxty: 95.98\n",
      "Epoch: 5, GlobalStep: 17681, step-time 1.22, Acc: 0.1857, Loss: 4.2764, Perpxty: 71.98\n",
      "Epoch: 5, GlobalStep: 17731, step-time 1.37, Acc: 0.1255, Loss: 5.2637, Perpxty: 193.19\n",
      "Epoch: 5, GlobalStep: 17781, step-time 1.36, Acc: 0.1830, Loss: 4.5111, Perpxty: 91.02\n",
      "Epoch: 5, GlobalStep: 17831, step-time 1.39, Acc: 0.2255, Loss: 4.6042, Perpxty: 99.90\n",
      "Epoch: 5, GlobalStep: 17881, step-time 1.64, Acc: 0.1348, Loss: 5.4526, Perpxty: 233.37\n",
      "Epoch: 5, GlobalStep: 17931, step-time 1.84, Acc: 0.1075, Loss: 5.3105, Perpxty: 202.44\n",
      "Epoch: 5, GlobalStep: 17981, step-time 1.81, Acc: 0.2533, Loss: 4.5533, Perpxty: 94.94\n",
      "Epoch: 5, GlobalStep: 18031, step-time 2.11, Acc: 0.1149, Loss: 5.3550, Perpxty: 211.66\n",
      "Time taken to save checkpoint:  3.546083450317383\n",
      "Epoch: 5, GlobalStep: 18081, step-time 2.52, Acc: 0.1388, Loss: 4.9688, Perpxty: 143.86\n",
      "at the end of epoch: 5\n",
      "Average train loss = 4.87266349, Average perplexity = 130.66848831\n",
      "Average train acc = 0.18771239\n",
      "validation loss = 5.35397404, perplexity = 211.44692865\n",
      "Average Validation acc = 0.14198770\n",
      "Time taken to save checkpoint:  3.4617512226104736\n",
      "Time taken to save checkpoint:  3.7335267066955566\n",
      "Epoch: 6, GlobalStep: 18097, step-time 0.59, Acc: 0.1769, Loss: 4.7953, Perpxty: 120.94\n",
      "Epoch: 6, GlobalStep: 18147, step-time 0.80, Acc: 0.2214, Loss: 4.5174, Perpxty: 91.59\n",
      "Epoch: 6, GlobalStep: 18197, step-time 0.81, Acc: 0.2000, Loss: 4.7448, Perpxty: 114.98\n",
      "Epoch: 6, GlobalStep: 18247, step-time 0.98, Acc: 0.1186, Loss: 5.4129, Perpxty: 224.28\n",
      "Epoch: 6, GlobalStep: 18297, step-time 0.98, Acc: 0.1931, Loss: 4.7093, Perpxty: 110.97\n",
      "Epoch: 6, GlobalStep: 18347, step-time 0.97, Acc: 0.2276, Loss: 5.0584, Perpxty: 157.34\n",
      "Epoch: 6, GlobalStep: 18397, step-time 0.98, Acc: 0.2522, Loss: 4.3523, Perpxty: 77.65\n",
      "Epoch: 6, GlobalStep: 18447, step-time 0.91, Acc: 0.2000, Loss: 4.6756, Perpxty: 107.30\n",
      "Epoch: 6, GlobalStep: 18497, step-time 0.95, Acc: 0.2231, Loss: 4.7001, Perpxty: 109.95\n",
      "Epoch: 6, GlobalStep: 18547, step-time 0.94, Acc: 0.2000, Loss: 4.6644, Perpxty: 106.10\n",
      "Epoch: 6, GlobalStep: 18597, step-time 0.94, Acc: 0.2231, Loss: 4.3779, Perpxty: 79.67\n",
      "Epoch: 6, GlobalStep: 18647, step-time 1.00, Acc: 0.2320, Loss: 4.5883, Perpxty: 98.33\n",
      "Epoch: 6, GlobalStep: 18697, step-time 0.91, Acc: 0.2545, Loss: 4.3991, Perpxty: 81.38\n",
      "Epoch: 6, GlobalStep: 18747, step-time 1.06, Acc: 0.2207, Loss: 4.6587, Perpxty: 105.50\n",
      "Epoch: 6, GlobalStep: 18797, step-time 1.00, Acc: 0.1931, Loss: 4.6480, Perpxty: 104.37\n",
      "Time taken to save checkpoint:  3.52432918548584\n",
      "Epoch: 6, GlobalStep: 18847, step-time 0.97, Acc: 0.1724, Loss: 4.7829, Perpxty: 119.45\n",
      "Epoch: 6, GlobalStep: 18897, step-time 1.05, Acc: 0.1956, Loss: 4.9290, Perpxty: 138.25\n",
      "Epoch: 6, GlobalStep: 18947, step-time 1.09, Acc: 0.1346, Loss: 5.3277, Perpxty: 205.96\n",
      "Epoch: 6, GlobalStep: 18997, step-time 1.09, Acc: 0.1733, Loss: 5.1212, Perpxty: 167.53\n",
      "Epoch: 6, GlobalStep: 19047, step-time 1.05, Acc: 0.1644, Loss: 4.3980, Perpxty: 81.29\n",
      "Epoch: 6, GlobalStep: 19097, step-time 1.11, Acc: 0.1444, Loss: 5.1819, Perpxty: 178.03\n",
      "Epoch: 6, GlobalStep: 19147, step-time 1.05, Acc: 0.1660, Loss: 5.0548, Perpxty: 156.77\n",
      "Epoch: 6, GlobalStep: 19197, step-time 1.05, Acc: 0.1600, Loss: 5.3716, Perpxty: 215.22\n",
      "Epoch: 6, GlobalStep: 19247, step-time 1.05, Acc: 0.1810, Loss: 4.9224, Perpxty: 137.33\n",
      "Epoch: 6, GlobalStep: 19297, step-time 1.11, Acc: 0.1321, Loss: 5.2158, Perpxty: 184.15\n",
      "Epoch: 6, GlobalStep: 19347, step-time 1.06, Acc: 0.2154, Loss: 4.3092, Perpxty: 74.38\n",
      "Epoch: 6, GlobalStep: 19397, step-time 1.12, Acc: 0.1862, Loss: 4.6959, Perpxty: 109.50\n",
      "Epoch: 6, GlobalStep: 19447, step-time 1.02, Acc: 0.1760, Loss: 5.1627, Perpxty: 174.64\n",
      "Epoch: 6, GlobalStep: 19497, step-time 1.11, Acc: 0.2000, Loss: 4.5727, Perpxty: 96.81\n",
      "Epoch: 6, GlobalStep: 19547, step-time 1.08, Acc: 0.2286, Loss: 4.5249, Perpxty: 92.29\n",
      "Time taken to save checkpoint:  3.4835803508758545\n",
      "Epoch: 6, GlobalStep: 19597, step-time 1.14, Acc: 0.1852, Loss: 4.6328, Perpxty: 102.80\n",
      "Epoch: 6, GlobalStep: 19647, step-time 1.02, Acc: 0.2160, Loss: 5.2256, Perpxty: 185.97\n",
      "Epoch: 6, GlobalStep: 19697, step-time 1.11, Acc: 0.2286, Loss: 4.3782, Perpxty: 79.70\n",
      "Epoch: 6, GlobalStep: 19747, step-time 1.12, Acc: 0.2000, Loss: 5.1652, Perpxty: 175.07\n",
      "Epoch: 6, GlobalStep: 19797, step-time 1.08, Acc: 0.3583, Loss: 3.6969, Perpxty: 40.32\n",
      "Epoch: 6, GlobalStep: 19847, step-time 1.08, Acc: 0.1714, Loss: 4.7440, Perpxty: 114.89\n",
      "Epoch: 6, GlobalStep: 19897, step-time 1.09, Acc: 0.2345, Loss: 4.1679, Perpxty: 64.58\n",
      "Epoch: 6, GlobalStep: 19947, step-time 1.06, Acc: 0.2080, Loss: 4.7676, Perpxty: 117.64\n",
      "Epoch: 6, GlobalStep: 19997, step-time 1.19, Acc: 0.1560, Loss: 4.9967, Perpxty: 147.93\n",
      "Epoch: 6, GlobalStep: 20047, step-time 1.27, Acc: 0.2189, Loss: 4.8493, Perpxty: 127.65\n",
      "Epoch: 6, GlobalStep: 20097, step-time 1.20, Acc: 0.1660, Loss: 4.8283, Perpxty: 125.00\n",
      "Epoch: 6, GlobalStep: 20147, step-time 1.31, Acc: 0.1429, Loss: 5.2471, Perpxty: 190.02\n",
      "Epoch: 6, GlobalStep: 20197, step-time 1.19, Acc: 0.1585, Loss: 5.0724, Perpxty: 159.56\n",
      "Epoch: 6, GlobalStep: 20247, step-time 1.19, Acc: 0.2308, Loss: 4.1780, Perpxty: 65.23\n",
      "Epoch: 6, GlobalStep: 20297, step-time 1.22, Acc: 0.1511, Loss: 4.5765, Perpxty: 97.18\n",
      "Time taken to save checkpoint:  3.4523065090179443\n",
      "Epoch: 6, GlobalStep: 20347, step-time 1.22, Acc: 0.1529, Loss: 4.8877, Perpxty: 132.65\n",
      "Epoch: 6, GlobalStep: 20397, step-time 1.28, Acc: 0.1690, Loss: 4.6806, Perpxty: 107.83\n",
      "Epoch: 6, GlobalStep: 20447, step-time 1.17, Acc: 0.1905, Loss: 4.8184, Perpxty: 123.76\n",
      "Epoch: 6, GlobalStep: 20497, step-time 1.19, Acc: 0.1800, Loss: 4.8483, Perpxty: 127.52\n",
      "Epoch: 6, GlobalStep: 20547, step-time 1.39, Acc: 0.1403, Loss: 5.1308, Perpxty: 169.15\n",
      "Epoch: 6, GlobalStep: 20597, step-time 1.16, Acc: 0.2286, Loss: 4.2757, Perpxty: 71.93\n",
      "Epoch: 6, GlobalStep: 20647, step-time 1.25, Acc: 0.2143, Loss: 4.4076, Perpxty: 82.07\n",
      "Epoch: 6, GlobalStep: 20697, step-time 1.25, Acc: 0.1786, Loss: 4.2786, Perpxty: 72.14\n",
      "Epoch: 6, GlobalStep: 20747, step-time 1.45, Acc: 0.1529, Loss: 5.1256, Perpxty: 168.27\n",
      "Epoch: 6, GlobalStep: 20797, step-time 1.31, Acc: 0.2128, Loss: 4.3281, Perpxty: 75.80\n",
      "Epoch: 6, GlobalStep: 20847, step-time 1.33, Acc: 0.2255, Loss: 4.4991, Perpxty: 89.94\n",
      "Epoch: 6, GlobalStep: 20897, step-time 1.69, Acc: 0.1630, Loss: 5.2863, Perpxty: 197.62\n",
      "Epoch: 6, GlobalStep: 20947, step-time 1.80, Acc: 0.1075, Loss: 5.1920, Perpxty: 179.83\n",
      "Epoch: 6, GlobalStep: 20997, step-time 1.78, Acc: 0.2489, Loss: 4.4342, Perpxty: 84.29\n",
      "Epoch: 6, GlobalStep: 21047, step-time 2.11, Acc: 0.0979, Loss: 5.3347, Perpxty: 207.40\n",
      "Time taken to save checkpoint:  3.4992189407348633\n",
      "Epoch: 6, GlobalStep: 21097, step-time 2.50, Acc: 0.1388, Loss: 4.8636, Perpxty: 129.50\n",
      "at the end of epoch: 6\n",
      "Average train loss = 4.75029838, Average perplexity = 115.61877804\n",
      "Average train acc = 0.19279143\n",
      "validation loss = 5.27016417, perplexity = 194.44788178\n",
      "Average Validation acc = 0.14896173\n",
      "Time taken to save checkpoint:  3.7459795475006104\n",
      "Time taken to save checkpoint:  3.48359751701355\n",
      "Epoch: 7, GlobalStep: 21113, step-time 0.62, Acc: 0.1923, Loss: 4.6035, Perpxty: 99.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, GlobalStep: 21163, step-time 0.80, Acc: 0.2286, Loss: 4.2560, Perpxty: 70.53\n",
      "Epoch: 7, GlobalStep: 21213, step-time 0.77, Acc: 0.2222, Loss: 4.5603, Perpxty: 95.61\n",
      "Epoch: 7, GlobalStep: 21263, step-time 0.98, Acc: 0.1322, Loss: 5.2695, Perpxty: 194.32\n",
      "Epoch: 7, GlobalStep: 21313, step-time 0.97, Acc: 0.2000, Loss: 4.5375, Perpxty: 93.45\n",
      "Epoch: 7, GlobalStep: 21363, step-time 1.02, Acc: 0.2276, Loss: 4.8301, Perpxty: 125.22\n",
      "Epoch: 7, GlobalStep: 21413, step-time 0.94, Acc: 0.2435, Loss: 4.3118, Perpxty: 74.57\n",
      "Epoch: 7, GlobalStep: 21463, step-time 0.94, Acc: 0.2160, Loss: 4.5620, Perpxty: 95.77\n",
      "Epoch: 7, GlobalStep: 21513, step-time 0.97, Acc: 0.2615, Loss: 4.4984, Perpxty: 89.87\n",
      "Epoch: 7, GlobalStep: 21563, step-time 0.95, Acc: 0.1750, Loss: 4.5250, Perpxty: 92.30\n",
      "Epoch: 7, GlobalStep: 21613, step-time 0.92, Acc: 0.2308, Loss: 4.1828, Perpxty: 65.55\n",
      "Epoch: 7, GlobalStep: 21663, step-time 0.94, Acc: 0.2160, Loss: 4.3427, Perpxty: 76.92\n",
      "Epoch: 7, GlobalStep: 21713, step-time 0.91, Acc: 0.2727, Loss: 4.2177, Perpxty: 67.87\n",
      "Epoch: 7, GlobalStep: 21763, step-time 0.97, Acc: 0.2276, Loss: 4.6053, Perpxty: 100.01\n",
      "Epoch: 7, GlobalStep: 21813, step-time 0.98, Acc: 0.2138, Loss: 4.6794, Perpxty: 107.71\n",
      "Time taken to save checkpoint:  3.5304431915283203\n",
      "Epoch: 7, GlobalStep: 21863, step-time 0.95, Acc: 0.1586, Loss: 4.7425, Perpxty: 114.72\n",
      "Epoch: 7, GlobalStep: 21913, step-time 1.08, Acc: 0.2222, Loss: 4.8216, Perpxty: 124.17\n",
      "Epoch: 7, GlobalStep: 21963, step-time 1.17, Acc: 0.1538, Loss: 5.2183, Perpxty: 184.61\n",
      "Epoch: 7, GlobalStep: 22013, step-time 1.08, Acc: 0.1911, Loss: 4.8691, Perpxty: 130.21\n",
      "Epoch: 7, GlobalStep: 22063, step-time 1.06, Acc: 0.1644, Loss: 4.3153, Perpxty: 74.84\n",
      "Epoch: 7, GlobalStep: 22113, step-time 1.11, Acc: 0.1741, Loss: 5.0593, Perpxty: 157.48\n",
      "Epoch: 7, GlobalStep: 22163, step-time 1.06, Acc: 0.1915, Loss: 4.8723, Perpxty: 130.62\n",
      "Epoch: 7, GlobalStep: 22213, step-time 1.03, Acc: 0.1778, Loss: 5.3061, Perpxty: 201.57\n",
      "Epoch: 7, GlobalStep: 22263, step-time 1.03, Acc: 0.2048, Loss: 4.7364, Perpxty: 114.02\n",
      "Epoch: 7, GlobalStep: 22313, step-time 1.12, Acc: 0.1321, Loss: 5.1431, Perpxty: 171.24\n",
      "Epoch: 7, GlobalStep: 22363, step-time 1.06, Acc: 0.2154, Loss: 4.1561, Perpxty: 63.82\n",
      "Epoch: 7, GlobalStep: 22413, step-time 1.17, Acc: 0.2345, Loss: 4.3009, Perpxty: 73.77\n",
      "Epoch: 7, GlobalStep: 22463, step-time 1.05, Acc: 0.1840, Loss: 5.0013, Perpxty: 148.61\n",
      "Epoch: 7, GlobalStep: 22513, step-time 1.12, Acc: 0.2593, Loss: 4.2906, Perpxty: 73.01\n",
      "Epoch: 7, GlobalStep: 22563, step-time 1.11, Acc: 0.2786, Loss: 4.2001, Perpxty: 66.69\n",
      "Time taken to save checkpoint:  3.3429579734802246\n",
      "Epoch: 7, GlobalStep: 22613, step-time 1.08, Acc: 0.2074, Loss: 4.5385, Perpxty: 93.55\n",
      "Epoch: 7, GlobalStep: 22663, step-time 1.03, Acc: 0.1920, Loss: 5.0970, Perpxty: 163.52\n",
      "Epoch: 7, GlobalStep: 22713, step-time 1.23, Acc: 0.2571, Loss: 4.2565, Perpxty: 70.56\n",
      "Epoch: 7, GlobalStep: 22763, step-time 1.19, Acc: 0.1857, Loss: 4.9655, Perpxty: 143.38\n",
      "Epoch: 7, GlobalStep: 22813, step-time 1.17, Acc: 0.3333, Loss: 3.3825, Perpxty: 29.45\n",
      "Epoch: 7, GlobalStep: 22863, step-time 1.09, Acc: 0.2143, Loss: 4.5485, Perpxty: 94.49\n",
      "Epoch: 7, GlobalStep: 22913, step-time 1.12, Acc: 0.3310, Loss: 3.8857, Perpxty: 48.70\n",
      "Epoch: 7, GlobalStep: 22963, step-time 1.05, Acc: 0.2400, Loss: 4.5533, Perpxty: 94.94\n",
      "Epoch: 7, GlobalStep: 23013, step-time 1.23, Acc: 0.1480, Loss: 4.7722, Perpxty: 118.18\n",
      "Epoch: 7, GlobalStep: 23063, step-time 1.28, Acc: 0.2189, Loss: 4.7815, Perpxty: 119.28\n",
      "Epoch: 7, GlobalStep: 23113, step-time 1.23, Acc: 0.1915, Loss: 4.6416, Perpxty: 103.71\n",
      "Epoch: 7, GlobalStep: 23163, step-time 1.25, Acc: 0.1571, Loss: 5.0763, Perpxty: 160.18\n",
      "Epoch: 7, GlobalStep: 23213, step-time 1.25, Acc: 0.1396, Loss: 4.7929, Perpxty: 120.65\n",
      "Epoch: 7, GlobalStep: 23263, step-time 1.17, Acc: 0.3333, Loss: 3.8890, Perpxty: 48.86\n",
      "Epoch: 7, GlobalStep: 23313, step-time 1.25, Acc: 0.1867, Loss: 4.3897, Perpxty: 80.61\n",
      "Time taken to save checkpoint:  3.3741836547851562\n",
      "Epoch: 7, GlobalStep: 23363, step-time 1.17, Acc: 0.1490, Loss: 4.7945, Perpxty: 120.84\n",
      "Epoch: 7, GlobalStep: 23413, step-time 1.25, Acc: 0.1828, Loss: 4.4262, Perpxty: 83.61\n",
      "Epoch: 7, GlobalStep: 23463, step-time 1.14, Acc: 0.1952, Loss: 4.6109, Perpxty: 100.58\n",
      "Epoch: 7, GlobalStep: 23513, step-time 1.12, Acc: 0.1850, Loss: 4.6316, Perpxty: 102.68\n",
      "Epoch: 7, GlobalStep: 23563, step-time 1.44, Acc: 0.1584, Loss: 4.8238, Perpxty: 124.44\n",
      "Epoch: 7, GlobalStep: 23613, step-time 1.14, Acc: 0.2643, Loss: 4.1122, Perpxty: 61.08\n",
      "Epoch: 7, GlobalStep: 23663, step-time 1.30, Acc: 0.2429, Loss: 4.1150, Perpxty: 61.25\n",
      "Epoch: 7, GlobalStep: 23713, step-time 1.25, Acc: 0.2500, Loss: 3.9608, Perpxty: 52.50\n",
      "Epoch: 7, GlobalStep: 23763, step-time 1.42, Acc: 0.1412, Loss: 4.9351, Perpxty: 139.08\n",
      "Epoch: 7, GlobalStep: 23813, step-time 1.31, Acc: 0.2468, Loss: 3.9190, Perpxty: 50.35\n",
      "Epoch: 7, GlobalStep: 23863, step-time 1.34, Acc: 0.2218, Loss: 4.3501, Perpxty: 77.49\n",
      "Epoch: 7, GlobalStep: 23913, step-time 1.70, Acc: 0.1761, Loss: 5.1015, Perpxty: 164.26\n",
      "Epoch: 7, GlobalStep: 23963, step-time 1.86, Acc: 0.1376, Loss: 4.9704, Perpxty: 144.08\n",
      "Epoch: 7, GlobalStep: 24013, step-time 1.89, Acc: 0.2622, Loss: 4.2224, Perpxty: 68.20\n",
      "Epoch: 7, GlobalStep: 24063, step-time 2.22, Acc: 0.1149, Loss: 5.1654, Perpxty: 175.10\n",
      "Time taken to save checkpoint:  3.577267646789551\n",
      "Epoch: 7, GlobalStep: 24113, step-time 2.58, Acc: 0.1671, Loss: 4.6956, Perpxty: 109.47\n",
      "at the end of epoch: 7\n",
      "Average train loss = 4.57675444, Average perplexity = 97.19841869\n",
      "Average train acc = 0.20946806\n",
      "validation loss = 5.08161065, perplexity = 161.03321513\n",
      "Average Validation acc = 0.17941279\n",
      "Time taken to save checkpoint:  3.796013593673706\n",
      "Time taken to save checkpoint:  3.4991695880889893\n",
      "Epoch: 8, GlobalStep: 24129, step-time 0.59, Acc: 0.1846, Loss: 4.4020, Perpxty: 81.62\n",
      "Epoch: 8, GlobalStep: 24179, step-time 0.81, Acc: 0.2857, Loss: 4.0773, Perpxty: 58.99\n",
      "Epoch: 8, GlobalStep: 24229, step-time 0.80, Acc: 0.2741, Loss: 4.3050, Perpxty: 74.07\n",
      "Epoch: 8, GlobalStep: 24279, step-time 1.03, Acc: 0.1661, Loss: 5.0764, Perpxty: 160.20\n",
      "Epoch: 8, GlobalStep: 24329, step-time 0.95, Acc: 0.2483, Loss: 4.1669, Perpxty: 64.51\n",
      "Epoch: 8, GlobalStep: 24379, step-time 0.95, Acc: 0.2552, Loss: 4.6109, Perpxty: 100.58\n",
      "Epoch: 8, GlobalStep: 24429, step-time 1.00, Acc: 0.2696, Loss: 3.9033, Perpxty: 49.57\n",
      "Epoch: 8, GlobalStep: 24479, step-time 0.91, Acc: 0.2800, Loss: 4.1980, Perpxty: 66.56\n",
      "Epoch: 8, GlobalStep: 24529, step-time 0.97, Acc: 0.3231, Loss: 4.1083, Perpxty: 60.84\n",
      "Epoch: 8, GlobalStep: 24579, step-time 0.98, Acc: 0.2417, Loss: 4.1873, Perpxty: 65.85\n",
      "Epoch: 8, GlobalStep: 24629, step-time 0.95, Acc: 0.2538, Loss: 3.9529, Perpxty: 52.09\n",
      "Epoch: 8, GlobalStep: 24679, step-time 0.94, Acc: 0.3120, Loss: 3.9520, Perpxty: 52.04\n",
      "Epoch: 8, GlobalStep: 24729, step-time 0.95, Acc: 0.2818, Loss: 3.8321, Perpxty: 46.16\n",
      "Epoch: 8, GlobalStep: 24779, step-time 1.00, Acc: 0.2690, Loss: 4.3204, Perpxty: 75.22\n",
      "Epoch: 8, GlobalStep: 24829, step-time 0.98, Acc: 0.2483, Loss: 4.3358, Perpxty: 76.39\n",
      "Time taken to save checkpoint:  3.2648680210113525\n",
      "Epoch: 8, GlobalStep: 24879, step-time 1.00, Acc: 0.1862, Loss: 4.7477, Perpxty: 115.32\n",
      "Epoch: 8, GlobalStep: 24929, step-time 1.03, Acc: 0.2311, Loss: 4.5057, Perpxty: 90.54\n",
      "Epoch: 8, GlobalStep: 24979, step-time 1.11, Acc: 0.1731, Loss: 4.9018, Perpxty: 134.53\n",
      "Epoch: 8, GlobalStep: 25029, step-time 1.08, Acc: 0.2267, Loss: 4.6851, Perpxty: 108.32\n",
      "Epoch: 8, GlobalStep: 25079, step-time 1.09, Acc: 0.2044, Loss: 4.2011, Perpxty: 66.76\n",
      "Epoch: 8, GlobalStep: 25129, step-time 1.14, Acc: 0.2111, Loss: 4.6523, Perpxty: 104.83\n",
      "Epoch: 8, GlobalStep: 25179, step-time 1.08, Acc: 0.2553, Loss: 4.5960, Perpxty: 99.09\n",
      "Epoch: 8, GlobalStep: 25229, step-time 1.08, Acc: 0.1911, Loss: 4.9036, Perpxty: 134.78\n",
      "Epoch: 8, GlobalStep: 25279, step-time 1.06, Acc: 0.2429, Loss: 4.4425, Perpxty: 84.98\n",
      "Epoch: 8, GlobalStep: 25329, step-time 1.14, Acc: 0.1500, Loss: 4.7493, Perpxty: 115.50\n",
      "Epoch: 8, GlobalStep: 25379, step-time 1.09, Acc: 0.2615, Loss: 4.0023, Perpxty: 54.72\n",
      "Epoch: 8, GlobalStep: 25429, step-time 1.19, Acc: 0.2621, Loss: 3.9925, Perpxty: 54.19\n",
      "Epoch: 8, GlobalStep: 25479, step-time 1.08, Acc: 0.2000, Loss: 4.8533, Perpxty: 128.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, GlobalStep: 25529, step-time 1.11, Acc: 0.3111, Loss: 4.0048, Perpxty: 54.86\n",
      "Epoch: 8, GlobalStep: 25579, step-time 1.09, Acc: 0.3429, Loss: 3.8213, Perpxty: 45.66\n",
      "Time taken to save checkpoint:  3.5773134231567383\n",
      "Epoch: 8, GlobalStep: 25629, step-time 1.17, Acc: 0.2370, Loss: 4.2778, Perpxty: 72.08\n",
      "Epoch: 8, GlobalStep: 25679, step-time 1.02, Acc: 0.2640, Loss: 5.0611, Perpxty: 157.76\n",
      "Epoch: 8, GlobalStep: 25729, step-time 1.09, Acc: 0.2714, Loss: 4.1295, Perpxty: 62.15\n",
      "Epoch: 8, GlobalStep: 25779, step-time 1.17, Acc: 0.2571, Loss: 4.5634, Perpxty: 95.91\n",
      "Epoch: 8, GlobalStep: 25829, step-time 1.11, Acc: 0.4167, Loss: 3.1524, Perpxty: 23.39\n",
      "Epoch: 8, GlobalStep: 25879, step-time 1.09, Acc: 0.2286, Loss: 4.3579, Perpxty: 78.09\n",
      "Epoch: 8, GlobalStep: 25929, step-time 1.09, Acc: 0.3517, Loss: 3.6345, Perpxty: 37.88\n",
      "Epoch: 8, GlobalStep: 25979, step-time 1.14, Acc: 0.3040, Loss: 4.0920, Perpxty: 59.86\n",
      "Epoch: 8, GlobalStep: 26029, step-time 1.19, Acc: 0.1760, Loss: 4.4122, Perpxty: 82.45\n",
      "Epoch: 8, GlobalStep: 26079, step-time 1.23, Acc: 0.2226, Loss: 4.6669, Perpxty: 106.36\n",
      "Epoch: 8, GlobalStep: 26129, step-time 1.30, Acc: 0.2723, Loss: 4.1345, Perpxty: 62.46\n",
      "Epoch: 8, GlobalStep: 26179, step-time 1.25, Acc: 0.2071, Loss: 4.6587, Perpxty: 105.49\n",
      "Epoch: 8, GlobalStep: 26229, step-time 1.22, Acc: 0.1698, Loss: 4.5962, Perpxty: 99.11\n",
      "Epoch: 8, GlobalStep: 26279, step-time 1.22, Acc: 0.3333, Loss: 3.6850, Perpxty: 39.85\n",
      "Epoch: 8, GlobalStep: 26329, step-time 1.27, Acc: 0.2311, Loss: 4.2575, Perpxty: 70.64\n",
      "Time taken to save checkpoint:  3.4835660457611084\n",
      "Epoch: 8, GlobalStep: 26379, step-time 1.27, Acc: 0.1804, Loss: 4.4818, Perpxty: 88.40\n",
      "Epoch: 8, GlobalStep: 26429, step-time 1.27, Acc: 0.2379, Loss: 4.1196, Perpxty: 61.53\n",
      "Epoch: 8, GlobalStep: 26479, step-time 1.19, Acc: 0.2571, Loss: 3.9669, Perpxty: 52.82\n",
      "Epoch: 8, GlobalStep: 26529, step-time 1.14, Acc: 0.2450, Loss: 4.3554, Perpxty: 77.89\n",
      "Epoch: 8, GlobalStep: 26579, step-time 1.37, Acc: 0.2468, Loss: 4.4275, Perpxty: 83.72\n",
      "Epoch: 8, GlobalStep: 26629, step-time 1.12, Acc: 0.3000, Loss: 3.6645, Perpxty: 39.04\n",
      "Epoch: 8, GlobalStep: 26679, step-time 1.25, Acc: 0.2786, Loss: 3.7878, Perpxty: 44.16\n",
      "Epoch: 8, GlobalStep: 26729, step-time 1.22, Acc: 0.3214, Loss: 3.4673, Perpxty: 32.05\n",
      "Epoch: 8, GlobalStep: 26779, step-time 1.39, Acc: 0.1843, Loss: 4.6924, Perpxty: 109.11\n",
      "Epoch: 8, GlobalStep: 26829, step-time 1.30, Acc: 0.3149, Loss: 3.5996, Perpxty: 36.58\n",
      "Epoch: 8, GlobalStep: 26879, step-time 1.37, Acc: 0.2800, Loss: 3.9952, Perpxty: 54.34\n",
      "Epoch: 8, GlobalStep: 26929, step-time 1.64, Acc: 0.2065, Loss: 4.8484, Perpxty: 127.53\n",
      "Epoch: 8, GlobalStep: 26979, step-time 1.83, Acc: 0.1914, Loss: 4.6216, Perpxty: 101.66\n",
      "Epoch: 8, GlobalStep: 27029, step-time 1.83, Acc: 0.2889, Loss: 3.8674, Perpxty: 47.82\n",
      "Epoch: 8, GlobalStep: 27079, step-time 2.12, Acc: 0.1660, Loss: 4.8612, Perpxty: 129.17\n",
      "Time taken to save checkpoint:  3.389848232269287\n",
      "Epoch: 8, GlobalStep: 27129, step-time 2.58, Acc: 0.2212, Loss: 4.2445, Perpxty: 69.72\n",
      "at the end of epoch: 8\n",
      "Average train loss = 4.25104647, Average perplexity = 70.17881373\n",
      "Average train acc = 0.25275719\n",
      "validation loss = 4.72685163, perplexity = 112.93942659\n",
      "Average Validation acc = 0.23794717\n",
      "Time taken to save checkpoint:  3.6398024559020996\n",
      "Time taken to save checkpoint:  3.6241862773895264\n",
      "Epoch: 9, GlobalStep: 27145, step-time 0.61, Acc: 0.2538, Loss: 4.2963, Perpxty: 73.43\n",
      "Epoch: 9, GlobalStep: 27195, step-time 0.81, Acc: 0.3143, Loss: 3.7161, Perpxty: 41.10\n",
      "Epoch: 9, GlobalStep: 27245, step-time 0.80, Acc: 0.3037, Loss: 3.8903, Perpxty: 48.92\n",
      "Epoch: 9, GlobalStep: 27295, step-time 1.02, Acc: 0.2102, Loss: 4.5747, Perpxty: 97.00\n",
      "Epoch: 9, GlobalStep: 27345, step-time 0.98, Acc: 0.3103, Loss: 3.5619, Perpxty: 35.23\n",
      "Epoch: 9, GlobalStep: 27395, step-time 1.02, Acc: 0.3034, Loss: 4.2369, Perpxty: 69.20\n",
      "Epoch: 9, GlobalStep: 27445, step-time 0.98, Acc: 0.3043, Loss: 3.6053, Perpxty: 36.79\n",
      "Epoch: 9, GlobalStep: 27495, step-time 0.92, Acc: 0.3200, Loss: 3.8031, Perpxty: 44.84\n",
      "Epoch: 9, GlobalStep: 27545, step-time 0.95, Acc: 0.3462, Loss: 3.8241, Perpxty: 45.79\n",
      "Epoch: 9, GlobalStep: 27595, step-time 0.94, Acc: 0.2667, Loss: 3.9720, Perpxty: 53.09\n",
      "Epoch: 9, GlobalStep: 27645, step-time 0.95, Acc: 0.3154, Loss: 3.7324, Perpxty: 41.78\n",
      "Epoch: 9, GlobalStep: 27695, step-time 0.95, Acc: 0.3360, Loss: 3.6469, Perpxty: 38.36\n",
      "Epoch: 9, GlobalStep: 27745, step-time 1.02, Acc: 0.3818, Loss: 3.4140, Perpxty: 30.39\n",
      "Epoch: 9, GlobalStep: 27795, step-time 1.02, Acc: 0.2828, Loss: 4.1670, Perpxty: 64.52\n",
      "Epoch: 9, GlobalStep: 27845, step-time 1.00, Acc: 0.3379, Loss: 3.7375, Perpxty: 41.99\n",
      "Time taken to save checkpoint:  3.5929338932037354\n",
      "Epoch: 9, GlobalStep: 27895, step-time 0.98, Acc: 0.2552, Loss: 4.1901, Perpxty: 66.03\n",
      "Epoch: 9, GlobalStep: 27945, step-time 1.06, Acc: 0.3067, Loss: 4.1011, Perpxty: 60.41\n",
      "Epoch: 9, GlobalStep: 27995, step-time 1.14, Acc: 0.2038, Loss: 4.5803, Perpxty: 97.54\n",
      "Epoch: 9, GlobalStep: 28045, step-time 1.11, Acc: 0.3067, Loss: 4.1806, Perpxty: 65.41\n",
      "Epoch: 9, GlobalStep: 28095, step-time 1.09, Acc: 0.2356, Loss: 3.9488, Perpxty: 51.87\n",
      "Epoch: 9, GlobalStep: 28145, step-time 1.16, Acc: 0.2704, Loss: 4.2258, Perpxty: 68.43\n",
      "Epoch: 9, GlobalStep: 28195, step-time 1.09, Acc: 0.2936, Loss: 4.0456, Perpxty: 57.15\n",
      "Epoch: 9, GlobalStep: 28245, step-time 1.02, Acc: 0.2489, Loss: 4.6912, Perpxty: 108.98\n",
      "Epoch: 9, GlobalStep: 28295, step-time 1.03, Acc: 0.3095, Loss: 3.9713, Perpxty: 53.05\n",
      "Epoch: 9, GlobalStep: 28345, step-time 1.17, Acc: 0.1857, Loss: 4.4676, Perpxty: 87.15\n",
      "Epoch: 9, GlobalStep: 28395, step-time 1.11, Acc: 0.3769, Loss: 3.3214, Perpxty: 27.70\n",
      "Epoch: 9, GlobalStep: 28445, step-time 1.14, Acc: 0.3310, Loss: 3.6882, Perpxty: 39.97\n",
      "Epoch: 9, GlobalStep: 28495, step-time 1.00, Acc: 0.2480, Loss: 4.3003, Perpxty: 73.72\n",
      "Epoch: 9, GlobalStep: 28545, step-time 1.17, Acc: 0.3778, Loss: 3.4871, Perpxty: 32.69\n",
      "Epoch: 9, GlobalStep: 28595, step-time 1.06, Acc: 0.4786, Loss: 3.1898, Perpxty: 24.28\n",
      "Time taken to save checkpoint:  3.3898508548736572\n",
      "Epoch: 9, GlobalStep: 28645, step-time 1.12, Acc: 0.3111, Loss: 3.7892, Perpxty: 44.22\n",
      "Epoch: 9, GlobalStep: 28695, step-time 0.98, Acc: 0.2560, Loss: 4.7649, Perpxty: 117.32\n",
      "Epoch: 9, GlobalStep: 28745, step-time 1.12, Acc: 0.3071, Loss: 3.7445, Perpxty: 42.29\n",
      "Epoch: 9, GlobalStep: 28795, step-time 1.11, Acc: 0.2571, Loss: 4.3207, Perpxty: 75.24\n",
      "Epoch: 9, GlobalStep: 28845, step-time 1.11, Acc: 0.4750, Loss: 2.8523, Perpxty: 17.33\n",
      "Epoch: 9, GlobalStep: 28895, step-time 1.09, Acc: 0.2786, Loss: 3.8309, Perpxty: 46.10\n",
      "Epoch: 9, GlobalStep: 28945, step-time 1.12, Acc: 0.3931, Loss: 3.2026, Perpxty: 24.60\n",
      "Epoch: 9, GlobalStep: 28995, step-time 1.11, Acc: 0.3280, Loss: 4.0234, Perpxty: 55.89\n",
      "Epoch: 9, GlobalStep: 29045, step-time 1.22, Acc: 0.2640, Loss: 3.8716, Perpxty: 48.02\n",
      "Epoch: 9, GlobalStep: 29095, step-time 1.23, Acc: 0.2830, Loss: 4.3677, Perpxty: 78.87\n",
      "Epoch: 9, GlobalStep: 29145, step-time 1.27, Acc: 0.3149, Loss: 3.6599, Perpxty: 38.86\n",
      "Epoch: 9, GlobalStep: 29195, step-time 1.27, Acc: 0.2464, Loss: 4.2908, Perpxty: 73.03\n",
      "Epoch: 9, GlobalStep: 29245, step-time 1.27, Acc: 0.2302, Loss: 4.2150, Perpxty: 67.69\n",
      "Epoch: 9, GlobalStep: 29295, step-time 1.20, Acc: 0.3692, Loss: 3.2880, Perpxty: 26.79\n",
      "Epoch: 9, GlobalStep: 29345, step-time 1.22, Acc: 0.2756, Loss: 3.6835, Perpxty: 39.79\n",
      "Time taken to save checkpoint:  3.4211113452911377\n",
      "Epoch: 9, GlobalStep: 29395, step-time 1.20, Acc: 0.2157, Loss: 4.1169, Perpxty: 61.37\n",
      "Epoch: 9, GlobalStep: 29445, step-time 1.33, Acc: 0.3069, Loss: 3.5342, Perpxty: 34.27\n",
      "Epoch: 9, GlobalStep: 29495, step-time 1.17, Acc: 0.3095, Loss: 3.8340, Perpxty: 46.25\n",
      "Epoch: 9, GlobalStep: 29545, step-time 1.16, Acc: 0.2700, Loss: 4.0508, Perpxty: 57.44\n",
      "Epoch: 9, GlobalStep: 29595, step-time 1.41, Acc: 0.2571, Loss: 4.0577, Perpxty: 57.84\n",
      "Epoch: 9, GlobalStep: 29645, step-time 1.14, Acc: 0.3786, Loss: 3.3490, Perpxty: 28.47\n",
      "Epoch: 9, GlobalStep: 29695, step-time 1.23, Acc: 0.3143, Loss: 3.5554, Perpxty: 35.00\n",
      "Epoch: 9, GlobalStep: 29745, step-time 1.19, Acc: 0.3286, Loss: 3.2498, Perpxty: 25.78\n",
      "Epoch: 9, GlobalStep: 29795, step-time 1.39, Acc: 0.2275, Loss: 4.1026, Perpxty: 60.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, GlobalStep: 29845, step-time 1.39, Acc: 0.3702, Loss: 3.2579, Perpxty: 25.99\n",
      "Epoch: 9, GlobalStep: 29895, step-time 1.37, Acc: 0.3455, Loss: 3.5240, Perpxty: 33.92\n",
      "Epoch: 9, GlobalStep: 29945, step-time 1.66, Acc: 0.2391, Loss: 4.4656, Perpxty: 86.97\n",
      "Epoch: 9, GlobalStep: 29995, step-time 1.86, Acc: 0.2065, Loss: 4.1819, Perpxty: 65.49\n",
      "Epoch: 9, GlobalStep: 30045, step-time 1.84, Acc: 0.3867, Loss: 3.2386, Perpxty: 25.50\n",
      "Epoch: 9, GlobalStep: 30095, step-time 2.14, Acc: 0.2128, Loss: 4.3964, Perpxty: 81.16\n",
      "Time taken to save checkpoint:  3.608557939529419\n",
      "Epoch: 9, GlobalStep: 30145, step-time 2.59, Acc: 0.2353, Loss: 3.8031, Perpxty: 44.84\n",
      "at the end of epoch: 9\n",
      "Average train loss = 3.86635033, Average perplexity = 47.76773107\n",
      "Average train acc = 0.29992347\n",
      "validation loss = 4.33362349, perplexity = 76.21996903\n",
      "Average Validation acc = 0.27994202\n",
      "Time taken to save checkpoint:  3.8272178173065186\n",
      "Time taken to save checkpoint:  3.499207019805908\n",
      "Epoch: 10, GlobalStep: 30161, step-time 0.61, Acc: 0.3308, Loss: 3.5590, Perpxty: 35.13\n",
      "Epoch: 10, GlobalStep: 30211, step-time 0.83, Acc: 0.3429, Loss: 3.6204, Perpxty: 37.35\n",
      "Epoch: 10, GlobalStep: 30261, step-time 0.80, Acc: 0.3926, Loss: 3.2195, Perpxty: 25.02\n",
      "Epoch: 10, GlobalStep: 30311, step-time 1.00, Acc: 0.2644, Loss: 4.1048, Perpxty: 60.63\n",
      "Epoch: 10, GlobalStep: 30361, step-time 0.98, Acc: 0.4069, Loss: 3.1081, Perpxty: 22.38\n",
      "Epoch: 10, GlobalStep: 30411, step-time 0.98, Acc: 0.3862, Loss: 3.5630, Perpxty: 35.27\n",
      "Epoch: 10, GlobalStep: 30461, step-time 0.97, Acc: 0.3652, Loss: 3.0752, Perpxty: 21.66\n",
      "Epoch: 10, GlobalStep: 30511, step-time 0.92, Acc: 0.3840, Loss: 3.6409, Perpxty: 38.13\n",
      "Epoch: 10, GlobalStep: 30561, step-time 0.97, Acc: 0.3769, Loss: 3.5977, Perpxty: 36.52\n",
      "Epoch: 10, GlobalStep: 30611, step-time 0.97, Acc: 0.2833, Loss: 3.8004, Perpxty: 44.72\n",
      "Epoch: 10, GlobalStep: 30661, step-time 1.00, Acc: 0.3923, Loss: 3.3243, Perpxty: 27.78\n",
      "Epoch: 10, GlobalStep: 30711, step-time 0.94, Acc: 0.4320, Loss: 3.3374, Perpxty: 28.15\n",
      "Epoch: 10, GlobalStep: 30761, step-time 0.92, Acc: 0.4091, Loss: 2.9417, Perpxty: 18.95\n",
      "Epoch: 10, GlobalStep: 30811, step-time 1.02, Acc: 0.2552, Loss: 3.8782, Perpxty: 48.34\n",
      "Epoch: 10, GlobalStep: 30861, step-time 0.98, Acc: 0.3310, Loss: 3.2684, Perpxty: 26.27\n",
      "Time taken to save checkpoint:  3.5148308277130127\n",
      "Epoch: 10, GlobalStep: 30911, step-time 1.06, Acc: 0.3103, Loss: 3.7547, Perpxty: 42.72\n",
      "Epoch: 10, GlobalStep: 30961, step-time 1.12, Acc: 0.4000, Loss: 3.5539, Perpxty: 34.95\n",
      "Epoch: 10, GlobalStep: 31011, step-time 1.14, Acc: 0.2308, Loss: 4.0496, Perpxty: 57.37\n",
      "Epoch: 10, GlobalStep: 31061, step-time 1.08, Acc: 0.4044, Loss: 3.4607, Perpxty: 31.84\n",
      "Epoch: 10, GlobalStep: 31111, step-time 1.05, Acc: 0.2800, Loss: 3.7509, Perpxty: 42.56\n",
      "Epoch: 10, GlobalStep: 31161, step-time 1.17, Acc: 0.3259, Loss: 3.6649, Perpxty: 39.05\n",
      "Epoch: 10, GlobalStep: 31211, step-time 1.14, Acc: 0.2894, Loss: 3.8576, Perpxty: 47.35\n",
      "Epoch: 10, GlobalStep: 31261, step-time 1.05, Acc: 0.2533, Loss: 4.3907, Perpxty: 80.70\n",
      "Epoch: 10, GlobalStep: 31311, step-time 1.06, Acc: 0.3714, Loss: 3.5484, Perpxty: 34.76\n",
      "Epoch: 10, GlobalStep: 31361, step-time 1.14, Acc: 0.2143, Loss: 4.2198, Perpxty: 68.02\n",
      "Epoch: 10, GlobalStep: 31411, step-time 1.05, Acc: 0.3923, Loss: 2.9002, Perpxty: 18.18\n",
      "Epoch: 10, GlobalStep: 31461, step-time 1.11, Acc: 0.3931, Loss: 3.1615, Perpxty: 23.61\n",
      "Epoch: 10, GlobalStep: 31511, step-time 1.05, Acc: 0.2880, Loss: 3.7808, Perpxty: 43.85\n",
      "Epoch: 10, GlobalStep: 31561, step-time 1.11, Acc: 0.3407, Loss: 3.1448, Perpxty: 23.21\n",
      "Epoch: 10, GlobalStep: 31611, step-time 1.09, Acc: 0.4786, Loss: 2.9764, Perpxty: 19.62\n",
      "Time taken to save checkpoint:  3.405470609664917\n",
      "Epoch: 10, GlobalStep: 31661, step-time 1.12, Acc: 0.3407, Loss: 3.9366, Perpxty: 51.25\n",
      "Epoch: 10, GlobalStep: 31711, step-time 1.06, Acc: 0.2400, Loss: 4.8004, Perpxty: 121.56\n",
      "Epoch: 10, GlobalStep: 31761, step-time 1.12, Acc: 0.3357, Loss: 3.3376, Perpxty: 28.15\n",
      "Epoch: 10, GlobalStep: 31811, step-time 1.08, Acc: 0.3000, Loss: 3.6481, Perpxty: 38.40\n",
      "Epoch: 10, GlobalStep: 31861, step-time 1.08, Acc: 0.4417, Loss: 2.7782, Perpxty: 16.09\n",
      "Epoch: 10, GlobalStep: 31911, step-time 1.12, Acc: 0.3214, Loss: 3.5595, Perpxty: 35.15\n",
      "Epoch: 10, GlobalStep: 31961, step-time 1.12, Acc: 0.4345, Loss: 2.7059, Perpxty: 14.97\n",
      "Epoch: 10, GlobalStep: 32011, step-time 1.05, Acc: 0.3600, Loss: 3.4565, Perpxty: 31.71\n",
      "Epoch: 10, GlobalStep: 32061, step-time 1.23, Acc: 0.2840, Loss: 3.5963, Perpxty: 36.46\n",
      "Epoch: 10, GlobalStep: 32111, step-time 1.25, Acc: 0.3283, Loss: 4.0752, Perpxty: 58.86\n",
      "Epoch: 10, GlobalStep: 32161, step-time 1.27, Acc: 0.3617, Loss: 3.4285, Perpxty: 30.83\n",
      "Epoch: 10, GlobalStep: 32211, step-time 1.25, Acc: 0.2821, Loss: 3.8653, Perpxty: 47.72\n",
      "Epoch: 10, GlobalStep: 32261, step-time 1.22, Acc: 0.2792, Loss: 3.6881, Perpxty: 39.97\n",
      "Epoch: 10, GlobalStep: 32311, step-time 1.22, Acc: 0.4513, Loss: 2.9251, Perpxty: 18.64\n",
      "Epoch: 10, GlobalStep: 32361, step-time 1.25, Acc: 0.3067, Loss: 3.6412, Perpxty: 38.14\n",
      "Time taken to save checkpoint:  3.3429677486419678\n",
      "Epoch: 10, GlobalStep: 32411, step-time 1.19, Acc: 0.2431, Loss: 3.9646, Perpxty: 52.70\n",
      "Epoch: 10, GlobalStep: 32461, step-time 1.31, Acc: 0.3448, Loss: 3.1171, Perpxty: 22.58\n",
      "Epoch: 10, GlobalStep: 32511, step-time 1.17, Acc: 0.3333, Loss: 3.3089, Perpxty: 27.35\n",
      "Epoch: 10, GlobalStep: 32561, step-time 1.16, Acc: 0.2850, Loss: 3.8475, Perpxty: 46.88\n",
      "Epoch: 10, GlobalStep: 32611, step-time 1.36, Acc: 0.3377, Loss: 3.6456, Perpxty: 38.31\n",
      "Epoch: 10, GlobalStep: 32661, step-time 1.16, Acc: 0.4357, Loss: 3.0074, Perpxty: 20.23\n",
      "Epoch: 10, GlobalStep: 32711, step-time 1.28, Acc: 0.3571, Loss: 3.4879, Perpxty: 32.72\n",
      "Epoch: 10, GlobalStep: 32761, step-time 1.20, Acc: 0.4143, Loss: 2.8822, Perpxty: 17.85\n",
      "Epoch: 10, GlobalStep: 32811, step-time 1.42, Acc: 0.2784, Loss: 3.7429, Perpxty: 42.22\n",
      "Epoch: 10, GlobalStep: 32861, step-time 1.36, Acc: 0.4000, Loss: 2.9236, Perpxty: 18.61\n",
      "Epoch: 10, GlobalStep: 32911, step-time 1.33, Acc: 0.3455, Loss: 3.3466, Perpxty: 28.40\n",
      "Epoch: 10, GlobalStep: 32961, step-time 1.67, Acc: 0.2826, Loss: 4.1564, Perpxty: 63.84\n",
      "Epoch: 10, GlobalStep: 33011, step-time 1.77, Acc: 0.2710, Loss: 3.9172, Perpxty: 50.26\n",
      "Epoch: 10, GlobalStep: 33061, step-time 1.87, Acc: 0.4622, Loss: 2.8291, Perpxty: 16.93\n",
      "Epoch: 10, GlobalStep: 33111, step-time 2.17, Acc: 0.2319, Loss: 4.2799, Perpxty: 72.24\n",
      "Time taken to save checkpoint:  3.5148277282714844\n",
      "Epoch: 10, GlobalStep: 33161, step-time 2.58, Acc: 0.2753, Loss: 3.3582, Perpxty: 28.74\n",
      "at the end of epoch: 10\n",
      "Average train loss = 3.53889355, Average perplexity = 34.42880441\n",
      "Average train acc = 0.33818133\n",
      "validation loss = 4.05851708, perplexity = 57.88840325\n",
      "Average Validation acc = 0.31573942\n",
      "Time taken to save checkpoint:  3.84287691116333\n",
      "Time taken to save checkpoint:  3.592937707901001\n",
      "Epoch: 11, GlobalStep: 33177, step-time 0.59, Acc: 0.3231, Loss: 3.3656, Perpxty: 28.95\n",
      "Epoch: 11, GlobalStep: 33227, step-time 0.81, Acc: 0.4071, Loss: 3.1631, Perpxty: 23.64\n",
      "Epoch: 11, GlobalStep: 33277, step-time 0.81, Acc: 0.4000, Loss: 2.9832, Perpxty: 19.75\n",
      "Epoch: 11, GlobalStep: 33327, step-time 1.03, Acc: 0.2949, Loss: 3.7396, Perpxty: 42.08\n",
      "Epoch: 11, GlobalStep: 33377, step-time 0.95, Acc: 0.4138, Loss: 3.0275, Perpxty: 20.65\n",
      "Epoch: 11, GlobalStep: 33427, step-time 1.00, Acc: 0.3517, Loss: 3.3813, Perpxty: 29.41\n",
      "Epoch: 11, GlobalStep: 33477, step-time 1.05, Acc: 0.4870, Loss: 2.6029, Perpxty: 13.50\n",
      "Epoch: 11, GlobalStep: 33527, step-time 0.97, Acc: 0.4000, Loss: 3.3841, Perpxty: 29.49\n",
      "Epoch: 11, GlobalStep: 33577, step-time 1.00, Acc: 0.4077, Loss: 3.3929, Perpxty: 29.75\n",
      "Epoch: 11, GlobalStep: 33627, step-time 0.94, Acc: 0.3583, Loss: 3.2277, Perpxty: 25.22\n",
      "Epoch: 11, GlobalStep: 33677, step-time 0.97, Acc: 0.4077, Loss: 3.2076, Perpxty: 24.72\n",
      "Epoch: 11, GlobalStep: 33727, step-time 0.92, Acc: 0.4800, Loss: 3.0932, Perpxty: 22.05\n",
      "Epoch: 11, GlobalStep: 33777, step-time 1.00, Acc: 0.4818, Loss: 2.7075, Perpxty: 14.99\n",
      "Epoch: 11, GlobalStep: 33827, step-time 0.98, Acc: 0.3379, Loss: 3.4811, Perpxty: 32.49\n",
      "Epoch: 11, GlobalStep: 33877, step-time 0.97, Acc: 0.4276, Loss: 2.8777, Perpxty: 17.77\n",
      "Time taken to save checkpoint:  3.5460596084594727\n",
      "Epoch: 11, GlobalStep: 33927, step-time 1.00, Acc: 0.3655, Loss: 3.4840, Perpxty: 32.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, GlobalStep: 33977, step-time 1.12, Acc: 0.4311, Loss: 3.2121, Perpxty: 24.83\n",
      "Epoch: 11, GlobalStep: 34027, step-time 1.11, Acc: 0.2385, Loss: 3.9851, Perpxty: 53.79\n",
      "Epoch: 11, GlobalStep: 34077, step-time 1.09, Acc: 0.4222, Loss: 3.2195, Perpxty: 25.01\n",
      "Epoch: 11, GlobalStep: 34127, step-time 1.09, Acc: 0.2800, Loss: 3.7070, Perpxty: 40.73\n",
      "Epoch: 11, GlobalStep: 34177, step-time 1.12, Acc: 0.3741, Loss: 3.3390, Perpxty: 28.19\n",
      "Epoch: 11, GlobalStep: 34227, step-time 1.09, Acc: 0.3915, Loss: 3.4643, Perpxty: 31.95\n",
      "Epoch: 11, GlobalStep: 34277, step-time 1.03, Acc: 0.3022, Loss: 4.0396, Perpxty: 56.80\n",
      "Epoch: 11, GlobalStep: 34327, step-time 1.03, Acc: 0.3810, Loss: 3.4409, Perpxty: 31.22\n",
      "Epoch: 11, GlobalStep: 34377, step-time 1.16, Acc: 0.2357, Loss: 4.1402, Perpxty: 62.82\n",
      "Epoch: 11, GlobalStep: 34427, step-time 1.05, Acc: 0.4154, Loss: 2.6299, Perpxty: 13.87\n",
      "Epoch: 11, GlobalStep: 34477, step-time 1.14, Acc: 0.4483, Loss: 2.7383, Perpxty: 15.46\n",
      "Epoch: 11, GlobalStep: 34527, step-time 1.09, Acc: 0.3600, Loss: 3.5500, Perpxty: 34.81\n",
      "Epoch: 11, GlobalStep: 34577, step-time 1.17, Acc: 0.4296, Loss: 3.0094, Perpxty: 20.28\n",
      "Epoch: 11, GlobalStep: 34627, step-time 1.09, Acc: 0.5500, Loss: 2.1374, Perpxty: 8.48\n",
      "Time taken to save checkpoint:  3.546070098876953\n",
      "Epoch: 11, GlobalStep: 34677, step-time 1.09, Acc: 0.3778, Loss: 3.4757, Perpxty: 32.32\n",
      "Epoch: 11, GlobalStep: 34727, step-time 1.02, Acc: 0.2240, Loss: 4.6504, Perpxty: 104.63\n",
      "Epoch: 11, GlobalStep: 34777, step-time 1.11, Acc: 0.3500, Loss: 3.4706, Perpxty: 32.16\n",
      "Epoch: 11, GlobalStep: 34827, step-time 1.11, Acc: 0.3214, Loss: 3.3920, Perpxty: 29.73\n",
      "Epoch: 11, GlobalStep: 34877, step-time 1.09, Acc: 0.4583, Loss: 2.4328, Perpxty: 11.39\n",
      "Epoch: 11, GlobalStep: 34927, step-time 1.09, Acc: 0.4000, Loss: 3.2137, Perpxty: 24.87\n",
      "Epoch: 11, GlobalStep: 34977, step-time 1.12, Acc: 0.5103, Loss: 2.2585, Perpxty: 9.57\n",
      "Epoch: 11, GlobalStep: 35027, step-time 1.09, Acc: 0.3520, Loss: 3.3267, Perpxty: 27.85\n",
      "Epoch: 11, GlobalStep: 35077, step-time 1.22, Acc: 0.3200, Loss: 3.2631, Perpxty: 26.13\n",
      "Epoch: 11, GlobalStep: 35127, step-time 1.28, Acc: 0.3472, Loss: 3.9529, Perpxty: 52.09\n",
      "Epoch: 11, GlobalStep: 35177, step-time 1.22, Acc: 0.4000, Loss: 3.0868, Perpxty: 21.91\n",
      "Epoch: 11, GlobalStep: 35227, step-time 1.30, Acc: 0.3071, Loss: 3.7425, Perpxty: 42.20\n",
      "Epoch: 11, GlobalStep: 35277, step-time 1.22, Acc: 0.2792, Loss: 3.5229, Perpxty: 33.88\n",
      "Epoch: 11, GlobalStep: 35327, step-time 1.23, Acc: 0.4410, Loss: 3.1350, Perpxty: 22.99\n",
      "Epoch: 11, GlobalStep: 35377, step-time 1.25, Acc: 0.3556, Loss: 3.2779, Perpxty: 26.52\n",
      "Time taken to save checkpoint:  3.686673402786255\n",
      "Epoch: 11, GlobalStep: 35427, step-time 1.22, Acc: 0.2980, Loss: 3.4876, Perpxty: 32.71\n",
      "Epoch: 11, GlobalStep: 35477, step-time 1.31, Acc: 0.3793, Loss: 2.9306, Perpxty: 18.74\n",
      "Epoch: 11, GlobalStep: 35527, step-time 1.19, Acc: 0.4286, Loss: 2.8548, Perpxty: 17.37\n",
      "Epoch: 11, GlobalStep: 35577, step-time 1.16, Acc: 0.3500, Loss: 3.5377, Perpxty: 34.39\n",
      "Epoch: 11, GlobalStep: 35627, step-time 1.42, Acc: 0.3299, Loss: 3.4780, Perpxty: 32.39\n",
      "Epoch: 11, GlobalStep: 35677, step-time 1.19, Acc: 0.3857, Loss: 3.1908, Perpxty: 24.31\n",
      "Epoch: 11, GlobalStep: 35727, step-time 1.23, Acc: 0.3929, Loss: 3.1418, Perpxty: 23.14\n",
      "Epoch: 11, GlobalStep: 35777, step-time 1.33, Acc: 0.3929, Loss: 2.7665, Perpxty: 15.90\n",
      "Epoch: 11, GlobalStep: 35827, step-time 1.44, Acc: 0.3137, Loss: 3.6144, Perpxty: 37.13\n",
      "Epoch: 11, GlobalStep: 35877, step-time 1.33, Acc: 0.4383, Loss: 2.7764, Perpxty: 16.06\n",
      "Epoch: 11, GlobalStep: 35927, step-time 1.39, Acc: 0.3964, Loss: 3.0169, Perpxty: 20.43\n",
      "Epoch: 11, GlobalStep: 35977, step-time 1.67, Acc: 0.2957, Loss: 3.9922, Perpxty: 54.17\n",
      "Epoch: 11, GlobalStep: 36027, step-time 1.84, Acc: 0.2796, Loss: 3.6062, Perpxty: 36.83\n",
      "Epoch: 11, GlobalStep: 36077, step-time 1.86, Acc: 0.4889, Loss: 2.5556, Perpxty: 12.88\n",
      "Epoch: 11, GlobalStep: 36127, step-time 2.23, Acc: 0.2596, Loss: 3.9956, Perpxty: 54.36\n",
      "Time taken to save checkpoint:  3.4523417949676514\n",
      "Epoch: 11, GlobalStep: 36177, step-time 2.55, Acc: 0.3082, Loss: 3.1252, Perpxty: 22.76\n",
      "at the end of epoch: 11\n",
      "Average train loss = 3.28282786, Average perplexity = 26.65103162\n",
      "Average train acc = 0.37074040\n",
      "validation loss = 3.77449080, perplexity = 43.57531424\n",
      "Average Validation acc = 0.34666640\n",
      "Time taken to save checkpoint:  3.8986082077026367\n",
      "Time taken to save checkpoint:  3.6554160118103027\n",
      "Epoch: 12, GlobalStep: 36193, step-time 0.59, Acc: 0.3923, Loss: 3.2086, Perpxty: 24.75\n",
      "Epoch: 12, GlobalStep: 36243, step-time 0.81, Acc: 0.3786, Loss: 3.2673, Perpxty: 26.24\n",
      "Epoch: 12, GlobalStep: 36293, step-time 0.80, Acc: 0.4667, Loss: 2.8934, Perpxty: 18.05\n",
      "Epoch: 12, GlobalStep: 36343, step-time 1.08, Acc: 0.3559, Loss: 3.4102, Perpxty: 30.27\n",
      "Epoch: 12, GlobalStep: 36393, step-time 0.98, Acc: 0.4759, Loss: 2.8573, Perpxty: 17.41\n",
      "Epoch: 12, GlobalStep: 36443, step-time 1.00, Acc: 0.3793, Loss: 2.9891, Perpxty: 19.87\n",
      "Epoch: 12, GlobalStep: 36493, step-time 0.97, Acc: 0.5478, Loss: 2.1097, Perpxty: 8.25\n",
      "Epoch: 12, GlobalStep: 36543, step-time 0.94, Acc: 0.4160, Loss: 3.0951, Perpxty: 22.09\n",
      "Epoch: 12, GlobalStep: 36593, step-time 0.97, Acc: 0.4231, Loss: 3.0001, Perpxty: 20.09\n",
      "Epoch: 12, GlobalStep: 36643, step-time 0.98, Acc: 0.3000, Loss: 3.6743, Perpxty: 39.42\n",
      "Epoch: 12, GlobalStep: 36693, step-time 1.00, Acc: 0.4846, Loss: 2.6792, Perpxty: 14.57\n",
      "Epoch: 12, GlobalStep: 36743, step-time 1.02, Acc: 0.5440, Loss: 2.4376, Perpxty: 11.45\n",
      "Epoch: 12, GlobalStep: 36793, step-time 0.97, Acc: 0.5727, Loss: 2.0562, Perpxty: 7.82\n",
      "Epoch: 12, GlobalStep: 36843, step-time 1.03, Acc: 0.4000, Loss: 3.2450, Perpxty: 25.66\n",
      "Epoch: 12, GlobalStep: 36893, step-time 1.00, Acc: 0.4000, Loss: 2.9052, Perpxty: 18.27\n",
      "Time taken to save checkpoint:  3.4523422718048096\n",
      "Epoch: 12, GlobalStep: 36943, step-time 1.05, Acc: 0.3931, Loss: 3.4881, Perpxty: 32.72\n",
      "Epoch: 12, GlobalStep: 36993, step-time 1.08, Acc: 0.4356, Loss: 3.0957, Perpxty: 22.10\n",
      "Epoch: 12, GlobalStep: 37043, step-time 1.14, Acc: 0.2577, Loss: 3.7461, Perpxty: 42.36\n",
      "Epoch: 12, GlobalStep: 37093, step-time 1.12, Acc: 0.4222, Loss: 3.1487, Perpxty: 23.31\n",
      "Epoch: 12, GlobalStep: 37143, step-time 1.16, Acc: 0.2933, Loss: 3.6061, Perpxty: 36.82\n",
      "Epoch: 12, GlobalStep: 37193, step-time 1.17, Acc: 0.3556, Loss: 3.3742, Perpxty: 29.20\n",
      "Epoch: 12, GlobalStep: 37243, step-time 1.09, Acc: 0.3617, Loss: 3.2209, Perpxty: 25.05\n",
      "Epoch: 12, GlobalStep: 37293, step-time 1.09, Acc: 0.3733, Loss: 3.7956, Perpxty: 44.51\n",
      "Epoch: 12, GlobalStep: 37343, step-time 1.05, Acc: 0.3952, Loss: 3.2646, Perpxty: 26.17\n",
      "Epoch: 12, GlobalStep: 37393, step-time 1.19, Acc: 0.2679, Loss: 3.7888, Perpxty: 44.21\n",
      "Epoch: 12, GlobalStep: 37443, step-time 1.08, Acc: 0.4923, Loss: 2.4041, Perpxty: 11.07\n",
      "Epoch: 12, GlobalStep: 37493, step-time 1.20, Acc: 0.4828, Loss: 2.5522, Perpxty: 12.84\n",
      "Epoch: 12, GlobalStep: 37543, step-time 1.05, Acc: 0.3920, Loss: 3.1911, Perpxty: 24.32\n",
      "Epoch: 12, GlobalStep: 37593, step-time 1.17, Acc: 0.4074, Loss: 2.7710, Perpxty: 15.97\n",
      "Epoch: 12, GlobalStep: 37643, step-time 1.11, Acc: 0.5571, Loss: 1.8921, Perpxty: 6.63\n",
      "Time taken to save checkpoint:  3.358607530593872\n",
      "Epoch: 12, GlobalStep: 37693, step-time 1.11, Acc: 0.4296, Loss: 2.9658, Perpxty: 19.41\n",
      "Epoch: 12, GlobalStep: 37743, step-time 1.06, Acc: 0.2400, Loss: 4.7168, Perpxty: 111.81\n",
      "Epoch: 12, GlobalStep: 37793, step-time 1.17, Acc: 0.3643, Loss: 3.2504, Perpxty: 25.80\n",
      "Epoch: 12, GlobalStep: 37843, step-time 1.12, Acc: 0.3643, Loss: 3.2453, Perpxty: 25.67\n",
      "Epoch: 12, GlobalStep: 37893, step-time 1.14, Acc: 0.5333, Loss: 2.3756, Perpxty: 10.76\n",
      "Epoch: 12, GlobalStep: 37943, step-time 1.12, Acc: 0.3429, Loss: 3.3326, Perpxty: 28.01\n",
      "Epoch: 12, GlobalStep: 37993, step-time 1.11, Acc: 0.4552, Loss: 2.6341, Perpxty: 13.93\n",
      "Epoch: 12, GlobalStep: 38043, step-time 1.06, Acc: 0.4160, Loss: 2.8388, Perpxty: 17.10\n",
      "Epoch: 12, GlobalStep: 38093, step-time 1.20, Acc: 0.3680, Loss: 2.8363, Perpxty: 17.05\n",
      "Epoch: 12, GlobalStep: 38143, step-time 1.25, Acc: 0.3245, Loss: 3.8333, Perpxty: 46.22\n",
      "Epoch: 12, GlobalStep: 38193, step-time 1.28, Acc: 0.4213, Loss: 2.9988, Perpxty: 20.06\n",
      "Epoch: 12, GlobalStep: 38243, step-time 1.25, Acc: 0.3214, Loss: 3.4171, Perpxty: 30.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, GlobalStep: 38293, step-time 1.28, Acc: 0.3132, Loss: 3.2191, Perpxty: 25.00\n",
      "Epoch: 12, GlobalStep: 38343, step-time 1.17, Acc: 0.4923, Loss: 2.5719, Perpxty: 13.09\n",
      "Epoch: 12, GlobalStep: 38393, step-time 1.23, Acc: 0.3156, Loss: 3.3035, Perpxty: 27.21\n",
      "Time taken to save checkpoint:  3.5616843700408936\n",
      "Epoch: 12, GlobalStep: 38443, step-time 1.20, Acc: 0.2902, Loss: 3.4728, Perpxty: 32.23\n",
      "Epoch: 12, GlobalStep: 38493, step-time 1.33, Acc: 0.3966, Loss: 2.7254, Perpxty: 15.26\n",
      "Epoch: 12, GlobalStep: 38543, step-time 1.19, Acc: 0.4048, Loss: 3.0009, Perpxty: 20.10\n",
      "Epoch: 12, GlobalStep: 38593, step-time 1.14, Acc: 0.3700, Loss: 3.4360, Perpxty: 31.06\n",
      "Epoch: 12, GlobalStep: 38643, step-time 1.34, Acc: 0.3792, Loss: 3.2616, Perpxty: 26.09\n",
      "Epoch: 12, GlobalStep: 38693, step-time 1.16, Acc: 0.4071, Loss: 2.9217, Perpxty: 18.57\n",
      "Epoch: 12, GlobalStep: 38743, step-time 1.28, Acc: 0.4357, Loss: 3.0217, Perpxty: 20.53\n",
      "Epoch: 12, GlobalStep: 38793, step-time 1.27, Acc: 0.4643, Loss: 2.5308, Perpxty: 12.56\n",
      "Epoch: 12, GlobalStep: 38843, step-time 1.48, Acc: 0.3686, Loss: 3.1611, Perpxty: 23.60\n",
      "Epoch: 12, GlobalStep: 38893, step-time 1.36, Acc: 0.4085, Loss: 2.7084, Perpxty: 15.00\n",
      "Epoch: 12, GlobalStep: 38943, step-time 1.41, Acc: 0.3891, Loss: 3.2074, Perpxty: 24.71\n",
      "Epoch: 12, GlobalStep: 38993, step-time 1.73, Acc: 0.3217, Loss: 3.8343, Perpxty: 46.26\n",
      "Epoch: 12, GlobalStep: 39043, step-time 1.81, Acc: 0.2860, Loss: 3.7446, Perpxty: 42.29\n",
      "Epoch: 12, GlobalStep: 39093, step-time 1.83, Acc: 0.4711, Loss: 2.7290, Perpxty: 15.32\n",
      "Epoch: 12, GlobalStep: 39143, step-time 2.22, Acc: 0.2830, Loss: 3.9729, Perpxty: 53.14\n",
      "Time taken to save checkpoint:  3.4272608757019043\n",
      "Epoch: 12, GlobalStep: 39193, step-time 2.64, Acc: 0.3459, Loss: 2.8188, Perpxty: 16.76\n",
      "at the end of epoch: 12\n",
      "Average train loss = 3.08228275, Average perplexity = 21.80812807\n",
      "Average train acc = 0.39687009\n",
      "validation loss = 3.66488052, perplexity = 39.05147013\n",
      "Average Validation acc = 0.35766136\n",
      "Time taken to save checkpoint:  3.7601773738861084\n",
      "Time taken to save checkpoint:  3.6072680950164795\n",
      "Epoch: 13, GlobalStep: 39209, step-time 0.59, Acc: 0.3692, Loss: 3.2051, Perpxty: 24.66\n",
      "Epoch: 13, GlobalStep: 39259, step-time 0.83, Acc: 0.4214, Loss: 2.8518, Perpxty: 17.32\n",
      "Epoch: 13, GlobalStep: 39309, step-time 0.85, Acc: 0.4889, Loss: 2.3456, Perpxty: 10.44\n",
      "Epoch: 13, GlobalStep: 39359, step-time 1.10, Acc: 0.3322, Loss: 3.4069, Perpxty: 30.17\n",
      "Epoch: 13, GlobalStep: 39409, step-time 0.98, Acc: 0.4690, Loss: 2.2100, Perpxty: 9.12\n",
      "Epoch: 13, GlobalStep: 39459, step-time 1.03, Acc: 0.4069, Loss: 2.9379, Perpxty: 18.88\n",
      "Epoch: 13, GlobalStep: 39509, step-time 0.96, Acc: 0.5043, Loss: 2.1023, Perpxty: 8.18\n",
      "Epoch: 13, GlobalStep: 39559, step-time 0.92, Acc: 0.4240, Loss: 2.9864, Perpxty: 19.81\n",
      "Epoch: 13, GlobalStep: 39609, step-time 0.98, Acc: 0.4308, Loss: 2.8206, Perpxty: 16.79\n",
      "Epoch: 13, GlobalStep: 39659, step-time 0.98, Acc: 0.3500, Loss: 3.2401, Perpxty: 25.54\n",
      "Epoch: 13, GlobalStep: 39709, step-time 0.98, Acc: 0.5231, Loss: 2.4364, Perpxty: 11.43\n",
      "Epoch: 13, GlobalStep: 39759, step-time 0.95, Acc: 0.5600, Loss: 2.4695, Perpxty: 11.82\n",
      "Epoch: 13, GlobalStep: 39809, step-time 0.94, Acc: 0.5818, Loss: 2.1843, Perpxty: 8.88\n",
      "Epoch: 13, GlobalStep: 39859, step-time 1.01, Acc: 0.3931, Loss: 3.1528, Perpxty: 23.40\n",
      "Epoch: 13, GlobalStep: 39909, step-time 0.99, Acc: 0.4207, Loss: 2.7270, Perpxty: 15.29\n",
      "Time taken to save checkpoint:  3.6314022541046143\n",
      "Epoch: 13, GlobalStep: 39959, step-time 1.02, Acc: 0.4000, Loss: 2.9387, Perpxty: 18.89\n",
      "Epoch: 13, GlobalStep: 40009, step-time 1.08, Acc: 0.4667, Loss: 3.0680, Perpxty: 21.50\n",
      "Epoch: 13, GlobalStep: 40059, step-time 1.11, Acc: 0.2731, Loss: 3.5821, Perpxty: 35.95\n",
      "Epoch: 13, GlobalStep: 40109, step-time 1.08, Acc: 0.4889, Loss: 2.6968, Perpxty: 14.83\n",
      "Epoch: 13, GlobalStep: 40159, step-time 1.12, Acc: 0.3200, Loss: 3.3434, Perpxty: 28.31\n",
      "Epoch: 13, GlobalStep: 40209, step-time 1.14, Acc: 0.3630, Loss: 3.2554, Perpxty: 25.93\n",
      "Epoch: 13, GlobalStep: 40259, step-time 1.10, Acc: 0.4255, Loss: 2.9872, Perpxty: 19.83\n",
      "Epoch: 13, GlobalStep: 40309, step-time 1.04, Acc: 0.3867, Loss: 3.6049, Perpxty: 36.78\n",
      "Epoch: 13, GlobalStep: 40359, step-time 1.04, Acc: 0.4286, Loss: 3.2668, Perpxty: 26.23\n",
      "Epoch: 13, GlobalStep: 40409, step-time 1.17, Acc: 0.3036, Loss: 3.6941, Perpxty: 40.21\n",
      "Epoch: 13, GlobalStep: 40459, step-time 1.05, Acc: 0.5077, Loss: 2.2513, Perpxty: 9.50\n",
      "Epoch: 13, GlobalStep: 40509, step-time 1.16, Acc: 0.5034, Loss: 2.4740, Perpxty: 11.87\n",
      "Epoch: 13, GlobalStep: 40559, step-time 1.04, Acc: 0.3760, Loss: 3.1491, Perpxty: 23.31\n",
      "Epoch: 13, GlobalStep: 40609, step-time 1.20, Acc: 0.4000, Loss: 2.8836, Perpxty: 17.88\n",
      "Epoch: 13, GlobalStep: 40659, step-time 1.17, Acc: 0.6429, Loss: 1.6304, Perpxty: 5.11\n",
      "Time taken to save checkpoint:  3.5014114379882812\n",
      "Epoch: 13, GlobalStep: 40709, step-time 1.13, Acc: 0.3704, Loss: 3.2138, Perpxty: 24.87\n",
      "Epoch: 13, GlobalStep: 40759, step-time 1.04, Acc: 0.2240, Loss: 4.7772, Perpxty: 118.77\n",
      "Epoch: 13, GlobalStep: 40809, step-time 1.14, Acc: 0.4286, Loss: 2.8618, Perpxty: 17.49\n",
      "Epoch: 13, GlobalStep: 40859, step-time 1.11, Acc: 0.4214, Loss: 2.8064, Perpxty: 16.55\n",
      "Epoch: 13, GlobalStep: 40909, step-time 1.12, Acc: 0.4917, Loss: 2.3056, Perpxty: 10.03\n",
      "Epoch: 13, GlobalStep: 40959, step-time 1.09, Acc: 0.4786, Loss: 2.8595, Perpxty: 17.45\n",
      "Epoch: 13, GlobalStep: 41009, step-time 1.09, Acc: 0.5103, Loss: 2.4237, Perpxty: 11.29\n",
      "Epoch: 13, GlobalStep: 41059, step-time 1.19, Acc: 0.3760, Loss: 3.1137, Perpxty: 22.50\n",
      "Epoch: 13, GlobalStep: 41109, step-time 1.19, Acc: 0.3440, Loss: 3.2500, Perpxty: 25.79\n",
      "Epoch: 13, GlobalStep: 41159, step-time 1.25, Acc: 0.4038, Loss: 3.4259, Perpxty: 30.75\n",
      "Epoch: 13, GlobalStep: 41209, step-time 1.30, Acc: 0.4809, Loss: 2.6801, Perpxty: 14.59\n",
      "Epoch: 13, GlobalStep: 41259, step-time 1.26, Acc: 0.3286, Loss: 3.1822, Perpxty: 24.10\n",
      "Epoch: 13, GlobalStep: 41309, step-time 1.22, Acc: 0.3208, Loss: 3.2733, Perpxty: 26.40\n",
      "Epoch: 13, GlobalStep: 41359, step-time 1.21, Acc: 0.4769, Loss: 2.5383, Perpxty: 12.66\n",
      "Epoch: 13, GlobalStep: 41409, step-time 1.25, Acc: 0.3511, Loss: 3.1798, Perpxty: 24.04\n",
      "Time taken to save checkpoint:  3.382215976715088\n",
      "Epoch: 13, GlobalStep: 41459, step-time 1.21, Acc: 0.2902, Loss: 3.5846, Perpxty: 36.04\n",
      "Epoch: 13, GlobalStep: 41509, step-time 1.30, Acc: 0.3793, Loss: 3.0316, Perpxty: 20.73\n",
      "Epoch: 13, GlobalStep: 41559, step-time 1.22, Acc: 0.4476, Loss: 2.7448, Perpxty: 15.56\n",
      "Epoch: 13, GlobalStep: 41609, step-time 1.15, Acc: 0.3600, Loss: 3.4693, Perpxty: 32.12\n",
      "Epoch: 13, GlobalStep: 41659, step-time 1.45, Acc: 0.3662, Loss: 3.2302, Perpxty: 25.28\n",
      "Epoch: 13, GlobalStep: 41709, step-time 1.19, Acc: 0.3929, Loss: 2.9515, Perpxty: 19.14\n",
      "Epoch: 13, GlobalStep: 41759, step-time 1.31, Acc: 0.3929, Loss: 2.8380, Perpxty: 17.08\n",
      "Epoch: 13, GlobalStep: 41809, step-time 1.25, Acc: 0.4571, Loss: 2.4897, Perpxty: 12.06\n",
      "Epoch: 13, GlobalStep: 41859, step-time 1.48, Acc: 0.3490, Loss: 3.2689, Perpxty: 26.28\n",
      "Epoch: 13, GlobalStep: 41909, step-time 1.40, Acc: 0.4128, Loss: 2.7414, Perpxty: 15.51\n",
      "Epoch: 13, GlobalStep: 41959, step-time 1.39, Acc: 0.4545, Loss: 2.9110, Perpxty: 18.38\n",
      "Epoch: 13, GlobalStep: 42009, step-time 1.64, Acc: 0.2978, Loss: 3.8903, Perpxty: 48.92\n",
      "Epoch: 13, GlobalStep: 42059, step-time 1.85, Acc: 0.2903, Loss: 3.6044, Perpxty: 36.76\n",
      "Epoch: 13, GlobalStep: 42109, step-time 1.81, Acc: 0.4889, Loss: 2.3973, Perpxty: 10.99\n",
      "Epoch: 13, GlobalStep: 42159, step-time 2.18, Acc: 0.2830, Loss: 3.8352, Perpxty: 46.30\n",
      "Time taken to save checkpoint:  3.5893285274505615\n",
      "Epoch: 13, GlobalStep: 42209, step-time 2.48, Acc: 0.3365, Loss: 3.0052, Perpxty: 20.19\n",
      "at the end of epoch: 13\n",
      "Average train loss = 3.00804233, Average perplexity = 20.24772273\n",
      "Average train acc = 0.40659877\n",
      "validation loss = 3.73602291, perplexity = 41.93089518\n",
      "Average Validation acc = 0.35154363\n",
      "Time taken to save checkpoint:  3.8114659786224365\n",
      "Time taken to save checkpoint:  3.409825086593628\n",
      "Epoch: 14, GlobalStep: 42225, step-time 0.61, Acc: 0.3615, Loss: 3.2604, Perpxty: 26.06\n",
      "Epoch: 14, GlobalStep: 42275, step-time 0.82, Acc: 0.4286, Loss: 3.0109, Perpxty: 20.31\n",
      "Epoch: 14, GlobalStep: 42325, step-time 0.93, Acc: 0.4815, Loss: 2.4551, Perpxty: 11.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, GlobalStep: 42375, step-time 1.04, Acc: 0.3254, Loss: 3.8566, Perpxty: 47.30\n",
      "Epoch: 14, GlobalStep: 42425, step-time 1.01, Acc: 0.5379, Loss: 2.1434, Perpxty: 8.53\n",
      "Epoch: 14, GlobalStep: 42475, step-time 1.00, Acc: 0.4138, Loss: 2.7585, Perpxty: 15.78\n",
      "Epoch: 14, GlobalStep: 42525, step-time 1.03, Acc: 0.5130, Loss: 2.1840, Perpxty: 8.88\n",
      "Epoch: 14, GlobalStep: 42575, step-time 0.95, Acc: 0.4240, Loss: 3.2414, Perpxty: 25.57\n",
      "Epoch: 14, GlobalStep: 42625, step-time 1.02, Acc: 0.4462, Loss: 2.7935, Perpxty: 16.34\n",
      "Epoch: 14, GlobalStep: 42675, step-time 1.05, Acc: 0.3750, Loss: 3.2268, Perpxty: 25.20\n",
      "Epoch: 14, GlobalStep: 42725, step-time 1.01, Acc: 0.4769, Loss: 2.7612, Perpxty: 15.82\n",
      "Epoch: 14, GlobalStep: 42775, step-time 1.00, Acc: 0.5680, Loss: 2.2763, Perpxty: 9.74\n",
      "Epoch: 14, GlobalStep: 42825, step-time 1.05, Acc: 0.6545, Loss: 2.0103, Perpxty: 7.47\n",
      "Epoch: 14, GlobalStep: 42875, step-time 1.03, Acc: 0.4345, Loss: 3.3000, Perpxty: 27.11\n",
      "Epoch: 14, GlobalStep: 42925, step-time 1.04, Acc: 0.4276, Loss: 2.5648, Perpxty: 13.00\n",
      "Time taken to save checkpoint:  3.6648638248443604\n",
      "Epoch: 14, GlobalStep: 42975, step-time 1.03, Acc: 0.4828, Loss: 2.6463, Perpxty: 14.10\n",
      "Epoch: 14, GlobalStep: 43025, step-time 1.11, Acc: 0.4400, Loss: 3.1670, Perpxty: 23.74\n",
      "Epoch: 14, GlobalStep: 43075, step-time 1.17, Acc: 0.2769, Loss: 3.7757, Perpxty: 43.63\n",
      "Epoch: 14, GlobalStep: 43125, step-time 1.13, Acc: 0.3556, Loss: 3.5614, Perpxty: 35.21\n",
      "Epoch: 14, GlobalStep: 43175, step-time 1.11, Acc: 0.2978, Loss: 3.7179, Perpxty: 41.18\n",
      "Epoch: 14, GlobalStep: 43225, step-time 1.17, Acc: 0.3630, Loss: 3.2192, Perpxty: 25.01\n",
      "Epoch: 14, GlobalStep: 43275, step-time 1.15, Acc: 0.3830, Loss: 3.2433, Perpxty: 25.62\n",
      "Epoch: 14, GlobalStep: 43325, step-time 1.11, Acc: 0.3556, Loss: 3.7456, Perpxty: 42.33\n",
      "Epoch: 14, GlobalStep: 43375, step-time 1.09, Acc: 0.4286, Loss: 3.2866, Perpxty: 26.75\n",
      "Epoch: 14, GlobalStep: 43425, step-time 1.25, Acc: 0.2464, Loss: 4.0719, Perpxty: 58.67\n",
      "Epoch: 14, GlobalStep: 43475, step-time 1.13, Acc: 0.5077, Loss: 2.3776, Perpxty: 10.78\n",
      "Epoch: 14, GlobalStep: 43525, step-time 1.22, Acc: 0.4621, Loss: 2.5607, Perpxty: 12.94\n",
      "Epoch: 14, GlobalStep: 43575, step-time 1.09, Acc: 0.3920, Loss: 3.0698, Perpxty: 21.54\n",
      "Epoch: 14, GlobalStep: 43625, step-time 1.18, Acc: 0.4667, Loss: 2.5503, Perpxty: 12.81\n",
      "Epoch: 14, GlobalStep: 43675, step-time 1.15, Acc: 0.6000, Loss: 1.6445, Perpxty: 5.18\n",
      "Time taken to save checkpoint:  3.3668110370635986\n",
      "Epoch: 14, GlobalStep: 43725, step-time 1.16, Acc: 0.3926, Loss: 3.0568, Perpxty: 21.26\n",
      "Epoch: 14, GlobalStep: 43775, step-time 1.09, Acc: 0.2400, Loss: 4.9554, Perpxty: 141.94\n",
      "Epoch: 14, GlobalStep: 43825, step-time 1.15, Acc: 0.4071, Loss: 2.9529, Perpxty: 19.16\n",
      "Epoch: 14, GlobalStep: 43875, step-time 1.21, Acc: 0.3786, Loss: 3.0533, Perpxty: 21.18\n",
      "Epoch: 14, GlobalStep: 43925, step-time 1.16, Acc: 0.5000, Loss: 2.7074, Perpxty: 14.99\n",
      "Epoch: 14, GlobalStep: 43975, step-time 1.18, Acc: 0.3714, Loss: 3.1032, Perpxty: 22.27\n",
      "Epoch: 14, GlobalStep: 44025, step-time 1.18, Acc: 0.4621, Loss: 2.9136, Perpxty: 18.42\n",
      "Epoch: 14, GlobalStep: 44075, step-time 1.16, Acc: 0.3760, Loss: 2.9004, Perpxty: 18.18\n",
      "Epoch: 14, GlobalStep: 44125, step-time 1.25, Acc: 0.3080, Loss: 3.6238, Perpxty: 37.48\n",
      "Epoch: 14, GlobalStep: 44175, step-time 1.28, Acc: 0.3509, Loss: 3.8015, Perpxty: 44.77\n",
      "Epoch: 14, GlobalStep: 44225, step-time 1.37, Acc: 0.3660, Loss: 3.3190, Perpxty: 27.63\n",
      "Epoch: 14, GlobalStep: 44275, step-time 1.26, Acc: 0.2929, Loss: 3.8855, Perpxty: 48.69\n",
      "Epoch: 14, GlobalStep: 44325, step-time 1.29, Acc: 0.2528, Loss: 3.6437, Perpxty: 38.23\n",
      "Epoch: 14, GlobalStep: 44375, step-time 1.18, Acc: 0.4205, Loss: 3.3478, Perpxty: 28.44\n",
      "Epoch: 14, GlobalStep: 44425, step-time 1.25, Acc: 0.2800, Loss: 3.9056, Perpxty: 49.68\n",
      "Time taken to save checkpoint:  3.548877477645874\n",
      "Epoch: 14, GlobalStep: 44475, step-time 1.28, Acc: 0.2510, Loss: 3.8827, Perpxty: 48.55\n",
      "Epoch: 14, GlobalStep: 44525, step-time 1.39, Acc: 0.3000, Loss: 3.4083, Perpxty: 30.21\n",
      "Epoch: 14, GlobalStep: 44575, step-time 1.20, Acc: 0.2667, Loss: 4.2290, Perpxty: 68.65\n",
      "Epoch: 14, GlobalStep: 44625, step-time 1.18, Acc: 0.2650, Loss: 4.4744, Perpxty: 87.74\n",
      "Epoch: 14, GlobalStep: 44675, step-time 1.40, Acc: 0.0857, Loss: 5.8801, Perpxty: 357.84\n",
      "Epoch: 14, GlobalStep: 44725, step-time 1.23, Acc: 0.2071, Loss: 4.8687, Perpxty: 130.16\n",
      "Epoch: 14, GlobalStep: 44775, step-time 1.27, Acc: 0.2000, Loss: 4.4201, Perpxty: 83.10\n",
      "Epoch: 14, GlobalStep: 44825, step-time 1.30, Acc: 0.2714, Loss: 3.8118, Perpxty: 45.23\n",
      "Epoch: 14, GlobalStep: 44875, step-time 1.39, Acc: 0.1412, Loss: 5.5360, Perpxty: 253.67\n"
     ]
    }
   ],
   "source": [
    "print (\"In Train\")\n",
    "try:\n",
    "    os.makedirs(FLAGS.train_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logging.info(\"Preparing summarization data.\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_docid, val_docid, train_queryid, val_queryid, train_sumid, val_sumid = train_test_split(docid, queryid, sumid, test_size=FLAGS.train_test_split, shuffle=False, random_state=42)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)    \n",
    "# please do not use the totality of the GPU memory\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "with tf.Graph().as_default(), tf.Session(config=config) as sess:\n",
    "    # tensorflow seed must be inside graph\n",
    "    tf.set_random_seed(FLAGS.seed)\n",
    "    np.random.seed(seed=FLAGS.seed)\n",
    "\n",
    "    # Create model.\n",
    "    logging.info(\"Creating %d layers of %d units.\" %\n",
    "                 (FLAGS.num_layers, FLAGS.size))\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.tfboard+'/train', sess.graph)\n",
    "    model = create_model(sess, doc_dict, sum_dict, False)\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    logging.info(\"Create buckets.\")\n",
    "    dev_set = create_bucket(val_docid, val_queryid, val_sumid)\n",
    "    train_set = create_bucket(train_docid, train_queryid, train_sumid)\n",
    "\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in range(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in range(len(train_bucket_sizes))]\n",
    "\n",
    "    for (s_size, q_size, t_size), nsample in zip(_buckets, train_bucket_sizes):\n",
    "        logging.info(\"Train set bucket ({}, {}, {}) has {} samples.\".format(\n",
    "            s_size, q_size, t_size, nsample))\n",
    "    batched_train_set = model.batchify(train_set, _buckets)\n",
    "    batched_dev_set = model.batchify(dev_set, _buckets)\n",
    "    # This is the training loop.\n",
    "    step_time, train_acc, train_loss = 0.0, 0.0, 0.0\n",
    "    step_start_time = 0\n",
    "    num_epoch = 0\n",
    "    step_time = 0\n",
    "    while num_epoch <= FLAGS.max_epochs:\n",
    "        epoch_train_loss = 0.0 \n",
    "        epoch_train_acc = 0.0\n",
    "        current_train_step = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for batch_train in batched_train_set:\n",
    "            \n",
    "            \n",
    "            step_start_time = time.time()                \n",
    "            encoder_doc_inputs, encoder_query_inputs, decoder_inputs, encoder_doc_len, encoder_query_len, decoder_len = batch_train\n",
    "            \n",
    "            step_train_loss, step_train_acc, _ = model.step(sess, encoder_doc_inputs, encoder_query_inputs, decoder_inputs,\n",
    "                encoder_doc_len, encoder_query_len, decoder_len, False, train_writer)\n",
    "            \n",
    "            step_time = time.time() - step_start_time\n",
    "            #print (\"CURRENT STEP: \", current_train_step, \" STEP TIME: \", step_time)\n",
    "    \n",
    "            step_train_loss =  (step_train_loss * FLAGS.batch_size)/np.sum(decoder_len)\n",
    "            epoch_train_loss += step_train_loss\n",
    "            epoch_train_acc += step_train_acc      \n",
    "\n",
    "            # Once in a while, we save checkpoint.\n",
    "            if current_train_step % FLAGS.steps_per_checkpoint == 0:\n",
    "                # Save checkpoint and zero timer and loss.\n",
    "                save_time_start = time.time()\n",
    "                checkpoint_path = os.path.join(FLAGS.train_dir, \"model.ckpt\")\n",
    "                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "                time_taken_to_save = time.time() - save_time_start\n",
    "                print(\"Time taken to save checkpoint: \", time_taken_to_save)\n",
    "\n",
    "            # Once in a while, we print statistics and run evals.\n",
    "            if current_train_step % FLAGS.steps_per_print == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                print (\"Epoch: %d, GlobalStep: %d, step-time %.2f, Acc: %.4f, Loss: %.4f, Perpxty: %.2f\" % (num_epoch, model.global_step.eval(), \n",
    "                                               step_time, \n",
    "                                               step_train_acc, \n",
    "                                               step_train_loss, \n",
    "                                               np.exp(float(step_train_loss))))\n",
    "                step_time, train_acc, train_loss = 0.0, 0.0, 0.0   \n",
    "            \n",
    "            current_train_step += 1\n",
    "\n",
    "        #epoch_train_loss, epoch_train_acc, current_train_step = 1., 2., 15\n",
    "        epoch_eval_loss, epoch_eval_acc = 0.0, 0.0\n",
    "        current_eval_step = 0\n",
    "        for batch_dev in batched_dev_set:\n",
    "            \n",
    "            encoder_doc_inputs, encoder_query_inputs, decoder_inputs, encoder_doc_len, encoder_query_len, decoder_len = batch_dev\n",
    "            step_eval_loss, step_eval_acc, _ = model.step(sess, encoder_doc_inputs,encoder_query_inputs,\n",
    "                                        decoder_inputs, encoder_doc_len, encoder_query_len,\n",
    "                                        decoder_len, True)\n",
    "            step_eval_loss = (step_eval_loss * FLAGS.batch_size) / np.sum(decoder_len)\n",
    "            epoch_eval_loss += step_eval_loss\n",
    "            epoch_eval_acc += step_eval_acc\n",
    "            current_eval_step += 1\n",
    "                \n",
    "        print(\"at the end of epoch:\", num_epoch)\n",
    "        print(\"Average train loss = %6.8f, Average perplexity = %6.8f\" % (epoch_train_loss/current_train_step, np.exp(epoch_train_loss/current_train_step)))\n",
    "        print(\"Average train acc = %6.8f\" % (epoch_train_acc/current_train_step))\n",
    "        print(\"validation loss = %6.8f, perplexity = %6.8f\" % (epoch_eval_loss/current_eval_step, np.exp(epoch_eval_loss/current_eval_step)))\n",
    "        print(\"Average Validation acc = %6.8f\" % (epoch_eval_acc/current_eval_step))\n",
    "\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        save_time_start = time.time()\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, \"model.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        time_taken_to_save = time.time() - save_time_start\n",
    "        print(\"Time taken to save checkpoint: \", time_taken_to_save)\n",
    "        num_epoch += 1\n",
    "            \n",
    "    sys.stdout.flush()\n",
    "\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode():\n",
    "    print (\"In Decode\")\n",
    "    # Load vocabularies.\n",
    "    doc_dict = load_dict(FLAGS.doc_dict_path)\n",
    "    sum_dict = load_dict(FLAGS.sum_dict_path)\n",
    "    if doc_dict is None or sum_dict is None:\n",
    "        logging.warning(\"Dict not found.\")\n",
    "    data = load_test_data(FLAGS.test_file, doc_dict)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Create model and load parameters.\n",
    "        logging.info(\"Creating %d layers of %d units.\" %\n",
    "                     (FLAGS.num_layers, FLAGS.size))\n",
    "        FLAGS.load_checkpoint = True\n",
    "        model = create_model(sess, doc_dict, sum_dict, True)\n",
    "        FLAGS.batch_size = 1\n",
    "        result = []\n",
    "        for idx, token_ids in enumerate(data):\n",
    "\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_doc_inputs, encoder_query_inputs, decoder_inputs, encoder_doc_len, encoder_query_len, decoder_len =\\\n",
    "                model.get_batch({0: [(token_ids, [ID_GO, ID_EOS])]}, 0)\n",
    "\n",
    "            if FLAGS.batch_size == 1 and FLAGS.geneos:\n",
    "                loss, outputs = model.step(sess,\n",
    "                    encoder_doc_inputs, encoder_query_inputs, decoder_inputs,\n",
    "                    encoder_doc_len, encoder_query_len, decoder_len, True)\n",
    "\n",
    "                outputs = [np.argmax(item) for item in outputs[0]]\n",
    "            else:\n",
    "                outputs = model.step_beam(\n",
    "                    sess, encoder_doc_inputs, encoder_query_inputs, encoder_doc_len, encoder_query_len, geneos=FLAGS.geneos)\n",
    "\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            if ID_EOS in outputs:\n",
    "                outputs = outputs[:outputs.index(ID_EOS)]\n",
    "            gen_sum = \" \".join(sen_map2tok(outputs, sum_dict[1]))\n",
    "            result.append(gen_sum)\n",
    "            logging.info(\"Finish {} samples. :: {}\".format(idx, gen_sum[:75]))\n",
    "        with open(FLAGS.test_output, \"w\") as f:\n",
    "            for item in result:\n",
    "                print(item, file=f)\n",
    "\n",
    "#decode(val_docid, val_queryid, val_sumid)\n",
    "print (\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
